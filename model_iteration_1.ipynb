{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll be building a predictive model for survival on the titanic based on training data provided by kaggle. This is part of the Warmup Project for Data Science 2016. \n",
    "\n",
    "#### A. import the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic = pandas.read_csv(\"./data/train.csv\")\n",
    "\n",
    "# Uncomment print statements below to take a look at the \n",
    "# first 5 rows of the dataframe and the describing output.\n",
    "# print(titanic.head(5))\n",
    "# print(titanic.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. clean up the missing data. \n",
    "\n",
    "Occasionally a dataset contains missing values (null, not a number, NA, etc.) and we want to prevent these missing values from affecting our computations in unintended ways. In particular, this training data set has missing values for `Age`, so let's clean that up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titanic[\"Age\"] = titanic[\"Age\"].fillna(titanic[\"Age\"].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. convert non-numeric (categorical) variables into usable numbers!\n",
    "\n",
    "In particular, `Sex` and `Embarked` should be converted into usable numbers. We'll find all the unique values for these non-numeric data points and replace them with numbers that can be used by the predictive model in a later step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique genders are ['male' 'female']\n"
     ]
    }
   ],
   "source": [
    "# Find all the unique genders \n",
    "print\"unique genders are\", titanic[\"Sex\"].unique()\n",
    "\n",
    "# From genders to numbers\n",
    "titanic.loc[titanic[\"Sex\"] == \"male\", \"Sex\"] = 0\n",
    "titanic.loc[titanic[\"Sex\"] == \"female\", \"Sex\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique embarked values are ['S' 'C' 'Q' nan]\n"
     ]
    }
   ],
   "source": [
    "# Find all the uniqued embarked values\n",
    "print \"unique embarked values are\", titanic[\"Embarked\"].unique()\n",
    "\n",
    "# From embarked letters to numbers\n",
    "titanic[\"Embarked\"] = titanic[\"Embarked\"].fillna(\"S\")\n",
    "titanic.loc[titanic[\"Embarked\"] == \"S\", \"Embarked\"] = 0\n",
    "titanic.loc[titanic[\"Embarked\"] == \"C\", \"Embarked\"] = 1\n",
    "titanic.loc[titanic[\"Embarked\"] == \"Q\", \"Embarked\"] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D. cross validation, linear regression, first stab at predictions \n",
    "\n",
    "We want to make sure that we don't train our model on the same data that we'll make predictions on, so we're going to split the data into several folds. In each trial, one fold will be set aside for predictions, and the remaining folds will be used for training. Thus there's no overlap between the folds/partitions that were used for training and the one fold used for predictions. We'll run several trials with these fold combinations and eventually get predictions for the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code from dataquest mission 74, part 9.\n",
    "\n",
    "# Import the linear regression class\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# Sklearn also has a helper that makes it easy to do cross validation\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "# The columns we'll use to predict the target\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "\n",
    "# Initialize our algorithm class\n",
    "alg = LinearRegression()\n",
    "# Generate cross validation folds for the titanic dataset.  It return the row indices corresponding to train and test.\n",
    "# We set random_state to ensure we get the same splits every time we run this.\n",
    "kf = KFold(titanic.shape[0], n_folds=3, random_state=1)\n",
    "\n",
    "predictions = []\n",
    "for train, test in kf:\n",
    "    # The predictors we're using the train the algorithm.  Note how we only take the rows in the train folds.\n",
    "    train_predictors = (titanic[predictors].iloc[train,:])\n",
    "    # The target we're using to train the algorithm.\n",
    "    train_target = titanic[\"Survived\"].iloc[train]\n",
    "    # Training the algorithm using the predictors and target.\n",
    "    alg.fit(train_predictors, train_target)\n",
    "    # We can now make predictions on the test fold\n",
    "    test_predictions = alg.predict(titanic[predictors].iloc[test,:])\n",
    "    predictions.append(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([  8.99877810e-02,   9.60756206e-01,   5.92676278e-01,\n",
      "         9.31138728e-01,   5.29343071e-02,   1.70275685e-01,\n",
      "         3.69943590e-01,   1.03474847e-01,   5.21597906e-01,\n",
      "         8.74491050e-01,   6.48883611e-01,   8.29742769e-01,\n",
      "         1.34797198e-01,  -1.61126844e-01,   6.58141307e-01,\n",
      "         6.39819748e-01,   1.51733875e-01,   2.95432718e-01,\n",
      "         5.35377959e-01,   6.21007683e-01,   2.61872592e-01,\n",
      "         2.62687561e-01,   7.31739160e-01,   5.05995897e-01,\n",
      "         5.61398567e-01,   3.35039734e-01,   1.30338808e-01,\n",
      "         4.68765767e-01,   6.60737753e-01,   9.10819218e-02,\n",
      "         4.77223920e-01,   1.04220026e+00,   6.60691613e-01,\n",
      "         8.71539273e-02,   5.28550732e-01,   4.01874338e-01,\n",
      "         1.30340307e-01,   1.29339672e-01,   5.72717129e-01,\n",
      "         6.65238822e-01,   4.83215779e-01,   7.60807408e-01,\n",
      "         1.30578363e-01,   8.71867121e-01,   7.09855487e-01,\n",
      "         9.11369897e-02,   1.39181745e-01,   6.60691613e-01,\n",
      "         6.82833485e-02,   6.06254374e-01,   4.92254383e-02,\n",
      "         1.29250392e-01,   9.02668258e-01,   7.51677954e-01,\n",
      "         3.19636822e-01,   5.05995897e-01,   8.23411477e-01,\n",
      "         1.27611544e-01,   8.16516947e-01,  -3.70209060e-02,\n",
      "         1.63085464e-01,   9.57981340e-01,   3.96742103e-01,\n",
      "         6.16138409e-02,   5.42714233e-01,   6.62112275e-02,\n",
      "         7.79751268e-01,   1.40293401e-01,   4.40592742e-01,\n",
      "         3.50534388e-02,   2.72709814e-01,   4.26360339e-01,\n",
      "         3.55241143e-01,   1.10226880e-01,   8.66078358e-02,\n",
      "         1.07366720e-01,   9.10819218e-02,   9.11369897e-02,\n",
      "         3.82661024e-01,   5.72471068e-01,   1.24221410e-01,\n",
      "         8.61972872e-02,   6.60705005e-01,   5.10138486e-01,\n",
      "         8.45241581e-01,   4.56477760e-01,   3.22699204e-02,\n",
      "         9.11369897e-02,   9.37604538e-01,   1.12967094e-01,\n",
      "         8.56794636e-02,   1.34727274e-01,   3.83320807e-01,\n",
      "         6.14970393e-03,  -7.83320148e-02,   9.11369897e-02,\n",
      "         3.10516665e-01,   5.49345421e-01,   7.23544338e-01,\n",
      "         2.33721448e-01,   5.81750798e-01,   9.10819218e-02,\n",
      "         5.25738424e-01,   6.40651310e-02,  -2.52427240e-02,\n",
      "         9.10819218e-02,   6.19865700e-01,   9.10387818e-02,\n",
      "         3.65066610e-02,   6.32939707e-01,   4.08195377e-01,\n",
      "         6.63657306e-01,   1.23882146e-01,   5.92491292e-01,\n",
      "         6.83623624e-01,   1.29295032e-01,  -6.19221217e-02,\n",
      "         2.59223480e-01,   6.09655955e-01,   5.30794378e-01,\n",
      "         2.88023805e-01,   9.11369897e-02,   2.82857942e-01,\n",
      "         7.61542726e-01,   3.45640063e-01,   1.85484998e-01,\n",
      "         1.70022737e-01,   1.12642722e-01,   5.59420117e-01,\n",
      "        -2.02485747e-03,   1.03290733e-01,   1.34440079e-01,\n",
      "         4.46807623e-01,   7.51677954e-01,   3.11805296e-01,\n",
      "         3.62947385e-01,   9.75724449e-01,   4.29554800e-01,\n",
      "         1.57043954e-01,   5.82928575e-01,   5.57105476e-01,\n",
      "         6.14443886e-01,   5.72812834e-01,   2.18783352e-01,\n",
      "         3.49472299e-01,   2.86040080e-01,   9.65037360e-02,\n",
      "         5.60916106e-01,   1.86919710e-01,   2.19027353e-01,\n",
      "         1.69739986e-01,   1.00690768e+00,  -5.89449777e-02,\n",
      "        -4.15452572e-02,   9.08736139e-02,   3.95827915e-01,\n",
      "         7.26175962e-01,   8.02219375e-02,   9.13557255e-02,\n",
      "        -2.22536096e-01,  -2.66919104e-02,   7.21593360e-01,\n",
      "         1.01953834e-01,   1.51388512e-01,   8.19705948e-02,\n",
      "         1.32518461e-01,   9.70245311e-01,   3.28974893e-01,\n",
      "         5.02576476e-01,   1.08437940e-01,   3.25183297e-01,\n",
      "         1.40818823e-01,   6.63268211e-01,   1.29295032e-01,\n",
      "         3.90965934e-01,   7.86503606e-02,  -3.68524682e-02,\n",
      "         9.13671691e-01,   2.84517666e-01,   4.46019673e-02,\n",
      "         2.68132779e-01,   3.35661255e-01,   1.96299597e-03,\n",
      "         3.51470400e-01,   6.51010647e-01,   5.11174133e-01,\n",
      "         6.29850621e-01,   4.10021732e-01,   4.03081359e-02,\n",
      "         4.74217131e-02,   7.64271489e-01,   3.44550453e-01,\n",
      "         5.97245007e-01,   3.69521460e-01,   9.46062691e-01,\n",
      "         9.12083149e-01,   1.70022737e-01,  -1.85251802e-02,\n",
      "         6.60691613e-01,   8.07931698e-01,   9.16548133e-02,\n",
      "        -2.22536096e-01,   5.78367977e-02,   3.48321010e-02,\n",
      "         1.45712251e-01,   6.91179799e-01,   3.84837497e-02,\n",
      "         1.45383056e-01,   7.26181926e-01,   4.78394987e-01,\n",
      "         1.12609974e-01,   7.50755869e-01,   1.23596450e-01,\n",
      "         2.84517666e-01,   1.36414068e-01,   1.01395495e+00,\n",
      "         5.87218752e-01,   1.90418359e-01,   1.02889863e+00,\n",
      "         2.83624866e-01,   1.56627303e-01,   3.00890244e-01,\n",
      "        -3.43861103e-02,   9.10819218e-02,   4.37274991e-01,\n",
      "         1.24346402e-01,   3.43657653e-01,   1.31782740e-01,\n",
      "         3.50007979e-01,   4.53816408e-01,   9.41986239e-01,\n",
      "         8.55812557e-02,   1.26427969e-01,   5.14461976e-01,\n",
      "         3.16370023e-01,   5.81627306e-01,   1.79146187e-01,\n",
      "         8.33217359e-01,   3.43657653e-01,   2.67886176e-01,\n",
      "         5.89980704e-01,   6.29850621e-01,   2.89082393e-01,\n",
      "         1.23551810e-01,   1.19423755e-01,   4.49914049e-01,\n",
      "         5.98080236e-01,   7.41700785e-01,   3.95976588e-01,\n",
      "         1.24570927e-01,   9.08512939e-02,   5.10217925e-01,\n",
      "         3.17243789e-01,   4.94880818e-02,   4.48434902e-01,\n",
      "         5.51647950e-01,   1.05176735e+00,   1.00396283e+00,\n",
      "         1.16824364e+00,   6.37295280e-01,   1.70022737e-01,\n",
      "         3.47081525e-02,   3.23790141e-01,   4.27827834e-01,\n",
      "         6.60691613e-01,   2.50879710e-01,   1.07703504e-04,\n",
      "         7.38026906e-02,   8.41682429e-01,   9.94221666e-01,\n",
      "         5.04388858e-01,   1.04634754e-01,   6.84091736e-01,\n",
      "         4.60920013e-01,   6.60691613e-01,   7.87205387e-01,\n",
      "         4.88920786e-01,   2.90790162e-01,   1.24446245e-01,\n",
      "         4.80968077e-01,  -3.19057282e-02,   9.10670657e-02,\n",
      "         1.57145126e-01,   1.40254724e-01,   5.02603260e-01,\n",
      "         1.03564537e-01,   8.07397611e-02,   1.23827078e-01,\n",
      "         2.19027353e-01,   6.93436769e-01,   1.02306096e+00,\n",
      "         1.07151871e+00,   2.91224311e-01,   6.03921666e-01,\n",
      "         1.12912026e-01,   5.42714233e-01,   1.54899175e-01]), array([ 1.13774791,  0.44173212,  0.98551347,  0.66915371,  0.08254228,\n",
      "        0.15142624,  0.83642014,  0.09704526,  0.64711481,  1.03845173,\n",
      "        1.06064212,  0.24647842,  0.98364902,  1.04411609,  1.10195734,\n",
      "        0.72596387,  0.09692709,  0.11388411,  0.60824987,  0.74905725,\n",
      "        0.090424  ,  1.00314273,  0.91588368,  0.13679886,  0.10365487,\n",
      "        0.82296458,  0.755174  , -0.27746285,  1.0035964 , -0.12636043,\n",
      "        0.70865678,  0.52438799,  1.06900476,  0.58044138,  0.32246331,\n",
      "        0.45904751,  0.0848131 ,  0.96838383,  0.09692709,  0.4123739 ,\n",
      "        0.96908901, -0.01732698,  0.33119158,  0.38953146,  0.97455471,\n",
      "        0.26457991,  0.28476325,  0.21075768,  0.78939013,  0.68174567,\n",
      "        0.5508181 ,  0.21132238,  0.00332574,  0.1315846 ,  0.44518065,\n",
      "        0.16116388,  0.07440511,  0.13363265,  0.09815645,  0.98913539,\n",
      "        0.69520122,  0.66925272,  0.66925272, -0.05732283,  0.25605759,\n",
      "        0.51306171,  0.04918447,  0.12689844,  0.08297663,  0.74556032,\n",
      "        0.63153497,  0.66915371,  1.03349593,  0.46795359,  0.11283671,\n",
      "        0.15759527,  0.5998862 ,  0.6125967 ,  0.96615292,  0.63469796,\n",
      "        0.6051113 ,  0.18499302,  0.15738453,  1.03364995,  0.80043282,\n",
      "        0.07003835,  0.85871777,  0.09692709,  0.37822123,  0.03771546,\n",
      "        0.70865678,  0.17123866,  0.87293786,  0.38692632,  0.14394491,\n",
      "       -0.00364112,  1.02362819,  0.60920867,  0.13721713,  0.57461098,\n",
      "        0.1534423 ,  0.29630296,  0.76221079,  0.0229439 ,  0.11050082,\n",
      "        0.59310377,  0.05272741,  0.64923598,  0.18004866, -0.05792355,\n",
      "        0.37724772,  0.14392897,  0.44776777,  0.09692709,  0.17057126,\n",
      "        0.97573347,  0.2546175 , -0.01069499,  0.59494436,  0.67712284,\n",
      "        0.81048116,  0.25112435,  0.7091068 ,  0.13414671,  0.21833626,\n",
      "        0.09018337,  0.5398775 ,  0.11371054,  0.09643219,  0.72214613,\n",
      "        0.83299143,  0.1712546 ,  0.07013414,  0.43870508,  0.5508181 ,\n",
      "        0.62795723,  0.17034196,  0.26289071,  1.03283656,  0.54234647,\n",
      "        0.66429253,  0.2888594 ,  0.24248073,  0.59832765,  0.15197868,\n",
      "        0.06672256,  0.76247901,  0.09709316,  0.62328105,  0.85873908,\n",
      "        0.39833841,  0.68526385,  0.28026543,  0.15249025,  0.0558822 ,\n",
      "        0.46338875,  0.3322838 ,  0.09704526,  0.12741893,  0.18977726,\n",
      "        0.90570685,  0.61255203,  0.1712546 ,  0.3041495 ,  0.05667859,\n",
      "        0.32003504,  0.13002433,  0.09704526,  0.02900113,  0.2546175 ,\n",
      "        0.25032727,  0.17123545,  0.71385691,  0.09643219,  0.03023685,\n",
      "        0.67057269,  0.83394424,  0.63668087,  0.45820842,  0.18004866,\n",
      "        0.03925263,  0.13700639,  0.76347615, -0.01610677,  0.2546175 ,\n",
      "       -0.05096587,  0.36065035,  0.49526401,  0.44776777,  0.88783867,\n",
      "        0.27650531,  0.0835897 ,  0.17095571,  0.0558822 ,  0.14352664,\n",
      "        0.26008209,  0.20422092,  0.14413971,  0.13917582,  0.78823881,\n",
      "        0.10244795,  0.983009  ,  0.12376157,  0.17152021,  0.71624816,\n",
      "        0.66906113,  0.5355726 ,  1.06327957,  0.55601524,  0.71952689,\n",
      "        0.43870508,  0.10813802,  0.14762674,  0.16452683,  0.09704526,\n",
      "        0.38468169,  0.77378051,  0.12353167,  0.31660245,  0.72019649,\n",
      "        0.18382257,  0.6683239 ,  0.07001598,  0.97445504,  0.13729376,\n",
      "        0.13363265,  0.88062695,  0.13363587,  0.08715737,  0.61255203,\n",
      "        0.5883169 ,  0.0229439 ,  0.18684089,  0.88743056,  0.13363587,\n",
      "        0.14770832,  0.62385335,  0.58195819,  0.89464072,  0.32433284,\n",
      "        1.0215796 ,  0.10198815,  1.01250232,  0.89757009,  0.52011358,\n",
      "        0.50665802,  0.19733591,  0.33882963,  0.19608356,  0.78269614,\n",
      "        0.3024605 ,  0.01303333,  0.35740293,  0.59528255,  0.2812701 ,\n",
      "        0.1713153 ,  0.17399933,  0.63510029,  0.2099606 ,  0.79897366,\n",
      "        0.62993975,  0.84335812,  0.49799211,  0.1712546 ,  0.01619374,\n",
      "        0.26496308,  0.09704526,  0.59494436,  0.03570385,  0.1574771 ,\n",
      "        0.55964686,  0.13363587,  0.0699841 ,  0.03391958,  0.68692335,\n",
      "        0.38475832,  0.66915371,  0.17777861,  0.16253816,  0.72211234,\n",
      "        0.83479538,  0.58677963,  0.07003835,  0.735757  ,  0.90451305,\n",
      "        0.09962007,  0.43250553,  0.13477258,  1.02529894,  0.13828479,\n",
      "        0.24105043,  0.13741193,  0.09704526,  0.04924194,  0.80169436,\n",
      "       -0.03139561,  0.64987806]), array([  1.72889219e-01,   1.70294715e-02,   7.82616935e-01,\n",
      "        -8.34788848e-03,   1.47022266e-01,   3.10888595e-01,\n",
      "         7.28261340e-01,   1.01479914e-01,   4.24565622e-01,\n",
      "         1.57316587e-02,   4.37708069e-01,   1.44204264e-02,\n",
      "         9.07678482e-02,   4.33913871e-01,   8.26537251e-01,\n",
      "         8.45262338e-01,   5.42776171e-01,   1.01763663e-01,\n",
      "         6.70148479e-01,   1.92163452e-01,   6.39359534e-02,\n",
      "         7.62650655e-01,   3.10124701e-02,   5.90024631e-01,\n",
      "         8.31356231e-01,   2.78648916e-01,   1.08309653e-01,\n",
      "         3.04531238e-01,   1.50864127e-01,   1.38986099e-01,\n",
      "         1.36219795e-01,   2.51197915e-01,   2.02625887e-01,\n",
      "         9.72357134e-01,   1.12191979e-01,   1.92169054e-01,\n",
      "         1.50211875e-01,  -2.14264992e-02,   4.52451020e-01,\n",
      "         4.38789988e-01,   6.04820088e-01,   7.89326541e-01,\n",
      "         8.00459867e-02,   2.10435721e-01,   5.70885269e-01,\n",
      "         5.70841743e-02,   1.44342132e-01,   1.00451104e+00,\n",
      "         6.42312317e-01,   8.51755703e-02,   7.33373007e-01,\n",
      "         3.09602117e-01,   1.49684208e-01,   3.22228832e-01,\n",
      "         1.01595923e-01,   6.50604478e-01,   1.01479914e-01,\n",
      "         8.45026241e-01,   1.38791822e-01,   7.14365273e-01,\n",
      "         7.68287651e-01,   1.84938938e-01,   1.01479914e-01,\n",
      "         6.54218524e-01,   2.93878313e-01,   2.96413137e-01,\n",
      "         1.92833539e-01,   8.27498735e-02,   3.28441263e-01,\n",
      "         5.87658439e-02,   1.02674988e-01,   1.42090676e-01,\n",
      "         2.83166248e-01,   1.01520440e-01,   2.10876914e-02,\n",
      "         9.01930011e-01,   6.80182444e-01,   3.63633521e-01,\n",
      "         4.29834748e-02,   2.51030051e-01,   2.71459394e-01,\n",
      "         1.55080767e-01,   1.20174297e-01,   6.76615822e-01,\n",
      "         5.21604336e-01,   2.74876851e-01,   7.14261845e-01,\n",
      "         4.63722197e-01,   1.43882255e-01,  -3.38493769e-02,\n",
      "         5.08333972e-02,   2.88240761e-01,   4.71949096e-03,\n",
      "         1.48920991e-01,   1.55073789e-01,   9.65241409e-01,\n",
      "         3.61956120e-01,   8.01212426e-01,   8.51755703e-02,\n",
      "         1.63090365e-01,   2.58489938e-01,   1.38385623e-01,\n",
      "         1.57316587e-02,   7.14397446e-01,   2.98282232e-01,\n",
      "         2.65779163e-02,   9.41922468e-01,   3.92478820e-01,\n",
      "         7.25879907e-01,   2.08234335e-01,   7.05625434e-02,\n",
      "         2.03820545e-01,   6.98106244e-01,   3.54986591e-01,\n",
      "         9.42312534e-01,   1.08182230e-01,   1.01115214e+00,\n",
      "         4.29882986e-01,   2.72580965e-01,   9.55913060e-02,\n",
      "         1.38553363e-01,   1.49766670e-01,   8.76445205e-01,\n",
      "         7.95521275e-01,   1.89563479e-01,   7.47402760e-02,\n",
      "         9.05943831e-01,   1.19035222e-01,   2.34961953e-01,\n",
      "         1.49265429e-01,   3.84688624e-01,   1.44070963e-01,\n",
      "         6.51000458e-01,   7.14396037e-01,   2.37161612e-01,\n",
      "         5.98123216e-01,   8.84762775e-01,   2.34195832e-01,\n",
      "         2.71459394e-01,   2.93878313e-01,   2.93878313e-01,\n",
      "         9.60495497e-02,   4.82543535e-01,   2.74738708e-01,\n",
      "         1.01479914e-01,   1.01479914e-01,   4.28725578e-01,\n",
      "         3.27845711e-01,   8.83507841e-01,   7.85083053e-02,\n",
      "         8.54020195e-02,   1.53868294e-01,   1.25458500e-01,\n",
      "         7.78614476e-01,   4.27536886e-01,   1.76095354e-01,\n",
      "         8.78367308e-01,   2.23270579e-01,   7.41615725e-02,\n",
      "         1.28260077e-01,   6.34105869e-01,   3.76826088e-01,\n",
      "         1.01513462e-01,   3.21161697e-01,   6.92919862e-02,\n",
      "         9.05219168e-01,   9.92643346e-02,   3.21100762e-02,\n",
      "         1.89869119e-01,   8.47257439e-01,   1.65792833e-01,\n",
      "         7.70032759e-01,   4.70822280e-01,   7.01001762e-01,\n",
      "         1.45018183e-01,   7.98992141e-02,   1.22365867e-01,\n",
      "        -5.62678525e-03,   6.34840292e-01,   1.47022266e-01,\n",
      "         6.21554022e-01,   1.55089154e-01,   1.92163452e-01,\n",
      "         7.45360827e-01,   1.92167645e-01,   8.15272492e-01,\n",
      "         7.49589740e-01,   9.59168970e-01,   4.23369546e-01,\n",
      "         6.56067455e-02,   1.17831761e-01,   1.17764665e-01,\n",
      "         6.77402825e-01,   1.31033823e-01,   2.11184136e-01,\n",
      "         3.61128670e-01,   1.92163452e-01,   3.27009298e-01,\n",
      "         2.80865752e-01,   4.73809464e-01,   1.17548012e-01,\n",
      "         2.08181789e-01,   8.39842956e-01,   6.07376016e-01,\n",
      "         1.36308792e-01,   5.71394060e-01,   2.34961953e-01,\n",
      "         7.32664113e-01,   4.58929866e-01,   2.99802486e-01,\n",
      "         1.07144857e-01,   8.54523415e-02,   3.79873628e-01,\n",
      "         6.77309159e-01,   2.08181789e-01,   8.74780819e-01,\n",
      "         1.12194764e-01,   3.71105893e-02,   2.30444621e-01,\n",
      "         5.78112549e-01,   8.80381008e-02,   4.38789988e-01,\n",
      "         6.50478673e-01,   2.52145211e-01,   2.16244600e-02,\n",
      "         7.72356638e-02,   7.64956968e-01,   1.06578734e-01,\n",
      "         3.85229660e-01,   6.33022282e-01,   6.89918839e-02,\n",
      "         1.92431836e-01,   8.51755703e-02,   4.59963761e-01,\n",
      "         1.92163452e-01,   7.52074841e-01,   6.94810438e-01,\n",
      "         3.74543331e-01,   1.47020857e-01,   1.28274033e-01,\n",
      "         1.54904640e-01,   8.83372143e-01,   1.38714930e-01,\n",
      "         1.01428183e-01,   6.37514393e-02,   4.74143535e-01,\n",
      "         1.44318380e-01,   3.32209243e-01,   9.85223737e-01,\n",
      "         1.12472244e-01,   1.60139061e-01,   2.66114644e-02,\n",
      "        -2.41362640e-01,   1.09304997e-01,   2.65882719e-01,\n",
      "         9.34799595e-01,   6.65962224e-02,  -1.44857067e-01,\n",
      "         7.32175244e-01,   1.01756702e+00,   6.57625381e-01,\n",
      "         6.82274953e-01,   7.78507074e-01,   3.06694232e-01,\n",
      "         7.03120381e-01,   1.47020857e-01,  -5.35194672e-02,\n",
      "         2.63450207e-01,   8.45198988e-01,   2.80865752e-01,\n",
      "         2.88522280e-01,   7.14342083e-01,   7.98068552e-01,\n",
      "         4.05781543e-01,   1.00941736e-01,   1.92789366e-01,\n",
      "         1.12191979e-01,   8.05473642e-01,   4.10332423e-01,\n",
      "        -6.55145848e-04,   7.89310178e-01,   7.38879084e-01,\n",
      "         1.43673989e-01,   1.49684208e-01,   1.01479914e-01,\n",
      "         8.33962978e-01,   8.06527571e-01,   7.46997500e-02,\n",
      "         6.54965242e-01,   2.67936850e-01,   1.17831761e-01,\n",
      "         6.75775470e-01,   2.72454182e-01,   9.99158265e-01,\n",
      "         5.87835137e-01,   4.84754956e-01,   1.70739321e-01])]\n"
     ]
    }
   ],
   "source": [
    "print predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D. contninued: accuracy!\n",
    "\n",
    "How did this first stab of predictions go? The possible outcomes are 1 and 0 (survival is a binary thing), but the linear regression model output doesn't match this binary format. Thus we have to map our predictions to outcomes. We'll also compute the accuracy of these results by comparing our predictions to the `Survived` column of the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The predictions are in three separate numpy arrays.  Concatenate them into one.  \n",
    "# We concatenate them on axis 0, as they only have one axis.\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# Map predictions to outcomes (only possible outcomes are 1 and 0)\n",
    "predictions[predictions > .5] = 1\n",
    "predictions[predictions <=.5] = 0\n",
    "\n",
    "# Take a look\n",
    "# print(predictions.shape)\n",
    "# print(titanic[\"Survived\"].shape)\n",
    "\n",
    "num_accurate_predictions = 0 # counter\n",
    "\n",
    "# Check whether the predictions are correct\n",
    "for i in range(predictions.shape[0]):\n",
    "    if predictions[i] == titanic[\"Survived\"][i]:\n",
    "        num_accurate_predictions +=1\n",
    "\n",
    "accuracy = float(num_accurate_predictions) / predictions.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of this linear regression model is `0.783389450056` -- definitely a lot of room for improvement! Perhaps using a different model or some feature engineering could help. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E. second stab: logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.787878787879\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize our algorithm\n",
    "alg = LogisticRegression(random_state=1)\n",
    "# Compute the accuracy score for all the cross validation folds.  (much simpler than what we did before!)\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=3)\n",
    "# Take the mean of the scores (because we have one for each fold)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the logistic regression model is `0.792368125701` -- better, but not perfect. Let's go through making a submission to kaggle before continuing to tweak the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F. preparing a submission to kaggle; running the model on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titanic_test = pandas.read_csv(\"./data/test.csv\")\n",
    "\n",
    "# Age column\n",
    "titanic_test[\"Age\"] = titanic_test[\"Age\"].fillna(titanic[\"Age\"].median())\n",
    "\n",
    "# Sex column\n",
    "titanic_test.loc[titanic_test[\"Sex\"] == \"male\", \"Sex\"] = 0\n",
    "titanic_test.loc[titanic_test[\"Sex\"] == \"female\", \"Sex\"] = 1\n",
    "\n",
    "# Embarked column\n",
    "titanic_test[\"Embarked\"] = titanic_test[\"Embarked\"].fillna(\"S\")\n",
    "titanic_test.loc[titanic_test[\"Embarked\"] == \"S\", \"Embarked\"] = 0\n",
    "titanic_test.loc[titanic_test[\"Embarked\"] == \"C\", \"Embarked\"] = 1\n",
    "titanic_test.loc[titanic_test[\"Embarked\"] == \"Q\", \"Embarked\"] = 2\n",
    "\n",
    "# Fare column\n",
    "titanic_test[\"Fare\"] = titanic_test[\"Fare\"].fillna(titanic[\"Fare\"].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the algorithm class\n",
    "alg = LogisticRegression(random_state=1)\n",
    "\n",
    "# Train the algorithm using all the training data\n",
    "alg.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "\n",
    "# Make predictions using the test set.\n",
    "predictions = alg.predict(titanic_test[predictors])\n",
    "\n",
    "# Create a new dataframe with only the columns Kaggle wants from the dataset.\n",
    "submission = pandas.DataFrame({\n",
    "        \"PassengerId\": titanic_test[\"PassengerId\"],\n",
    "        \"Survived\": predictions\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate a submission file\n",
    "# commented out to prevent unintentional file overwrite/creation\n",
    "# submission.to_csv(\"dataquest_logistic_regression.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### G. improving the dataquest code\n",
    "\n",
    "Brain dump of ideas:\n",
    "* Not using every feature in the model, relevant to the curse of dimensionality -- see if using the same logistic regression with less features is helpful. Perhaps things like ticket number and fare are not as useful as sex and age. \n",
    "* Try different models\n",
    "* Combine features together: perhaps combining sex and age into one feature somehow (encoding it with one digit for sex and one digit for age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Helper functions: Use logistic regression, try using different features\n",
    "\n",
    "def make_titanic_test_predictions(predictors):\n",
    "    # Initialize our algorithm\n",
    "    alg = LogisticRegression(random_state=1)\n",
    "    # Compute the accuracy score for all the cross validation folds.  (much simpler than what we did before!)\n",
    "    scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=3)\n",
    "    # Take the mean of the scores (because we have one for each fold)\n",
    "    print \"accuracy\", scores.mean()\n",
    "    return  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our first attempt, predictors included all of the provided features from the kaggle dataset:      \n",
    "`['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']`.    \n",
    "\n",
    "Let's see what happens when we do something super bare bones with just `Sex` and `Age`. I expect that this will be less accurate because while these features do seem important, there is probably more to the relationship between people and survival than `Sex and Age`.     \n",
    "\n",
    "The code in the next few cells somewhat resembles one of the data mining approaches in the reading (I believe the reading mentioned computing the correlation coefficient for each of the variables). We'll see which variables work well for predictions, and then proceeding onwards based on which variables seem to be helping the accuracy score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sex', 'Age']\n",
      "accuracy 0.786756453423\n"
     ]
    }
   ],
   "source": [
    "predictors2 = ['Sex', 'Age'] \n",
    "print predictors2\n",
    "predictions2 = make_titanic_test_predictions(predictors2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that using just `Sex` and `Age` gives us a score comparable to using all of the features! This definitely makes me think that some of the features in the dataset are not helpful in this logistic regression model... this is not a surprise because we know that more variables is not necessarily better with a fixed amount of data (insert reference to the curse of dimensionality concept.      \n",
    "\n",
    "Based on contextual knowledge about the Titanic story (DataQuest mission 74 also mentions this), we know that passenger class was relevant because the first class cabins were closer to the deck of the ship. A distance advantage to safety almost certainly would impact survival rate, so let's try including `Pclass` in addition to the bare-bones model based on just `Sex` and `Age`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pclass', 'Sex', 'Age']\n",
      "accuracy 0.789001122334\n"
     ]
    }
   ],
   "source": [
    "predictors3 = ['Pclass', 'Sex', 'Age']\n",
    "print predictors3\n",
    "predictions3 = make_titanic_test_predictions(predictors3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H. Other things to try (for model_iteration_2.ipynb) !\n",
    "\n",
    "Due to time constraints I didn't have a bunch of time to implement more ideas -- but these are some things I will explore more in future iterations and perhaps discuss in class soon:\n",
    "\n",
    "* Take another look at the data, see what the unique values themselves look like. For example, is there some pattern in the names of the passengers?\n",
    "* Combine variables:\n",
    "    * In the brain dump cell earlier I mentioned combining `sex` and `age` somehow. Consider \"female child, male child, female adult, male adult, female senior, male senior\", and put these categories in one variable. Maybe this would help the curse of dimensionality problem? Or maybe it would prevent the model from doing  \n",
    "* Consider the tradeoff between doing a bunch of feature engineering myself and letting the model figure out the trends on its own. There must be a sweet spot between the data processing I do and what happens automatically in logistic regression.\n",
    "* Revisit exploration.ipynb for more bottom-up data inspiration!\n",
    "* Different models provided by scikit-learn (Random Forest?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
