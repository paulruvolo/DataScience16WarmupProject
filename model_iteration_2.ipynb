{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Iteration 2\n",
    "In this notebook I explore several model types as inspired by resources which will be cited throughout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my first iteration, I was able to achieve a score of just over 78%. In this first run of trying different model types, I complete the DataQuest tutorial on random forests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#First, let's upload everything, get the data sets to \n",
    "#some level of functionality\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import re\n",
    "import operator\n",
    "%matplotlib inline\n",
    "\n",
    "titanic_master = pd.read_csv('data/train.csv', dtype={'Age':np.float64})\n",
    "test_data = pd.read_csv('data/test.csv', dtype={'Age':np.float64})\n",
    "\n",
    "#now for filling in the missing data in the two datasets\n",
    "titanic_master['Age'] = titanic_master['Age'].fillna(titanic_master['Age'].median())\n",
    "titanic_master['Fare'] = titanic_master['Fare'].fillna(titanic_master['Fare'].median())\n",
    "titanic_master.loc[titanic_master['Sex'] == 'male', 'Sex'] = 0\n",
    "titanic_master.loc[titanic_master['Sex'] == 'female', 'Sex'] = 1\n",
    "titanic_master['Embarked'] = titanic_master['Embarked'].fillna('S')\n",
    "titanic_master.loc[titanic_master['Embarked'] == 'S', 'Embarked'] = 0\n",
    "titanic_master.loc[titanic_master['Embarked'] == 'C', 'Embarked'] = 1\n",
    "titanic_master.loc[titanic_master['Embarked'] == 'Q', 'Embarked'] = 2\n",
    "\n",
    "test_data['Age'] = test_data['Age'].fillna(titanic_master['Age'].median())\n",
    "test_data['Fare'] = test_data['Fare'].fillna(titanic_master['Fare'].median())\n",
    "test_data.loc[test_data['Sex'] == 'male', 'Sex'] = 0\n",
    "test_data.loc[test_data['Sex'] == 'female', 'Sex'] = 1\n",
    "test_data['Embarked'] = test_data['Embarked'].fillna('S')\n",
    "test_data.loc[test_data['Embarked'] == 'S', 'Embarked'] = 0\n",
    "test_data.loc[test_data['Embarked'] == 'C', 'Embarked'] = 1\n",
    "test_data.loc[test_data['Embarked'] == 'Q', 'Embarked'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.820426487093\n"
     ]
    }
   ],
   "source": [
    "#now, to get into the suggestions from Dataquest...\n",
    "predictors = ['Pclass','Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=4, min_samples_leaf=2)\n",
    "kf = cross_validation.KFold(titanic_master.shape[0], n_folds=3, random_state=1)\n",
    "scores = cross_validation.cross_val_score(alg,titanic_master[predictors],titanic_master['Survived'],cv=3)\n",
    "print scores.mean()          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#I'm going to pause here and actually just apply this to the test data and submit this.\n",
    "alg.fit(titanic_master[predictors], titanic_master['Survived'])\n",
    "predictions = alg.predict_proba(test_data[predictors].astype(float))[:,1]\n",
    "\n",
    "predictions[predictions <= 0.5] = 0\n",
    "predictions[predictions > 0.5] = 1\n",
    "\n",
    "final_pred = []\n",
    "for element in predictions:\n",
    "    final_pred.append(int(element))\n",
    "\n",
    "submission = pd.DataFrame({'PassengerId': test_data['PassengerId'], 'Survived':final_pred})\n",
    "submission.to_csv('kaggle_v2_randforest.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This random forest implementation achieves a score of 0.76077, which is not as compelling as I was able to achieve in my first model iteration. Thinking further ahead (and as suggested by the Dataquest tutorial as well as [this blog](http://elenacuoco.altervista.org/blog/archives/1195) I create different data types to explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#family size\n",
    "titanic_master['Famsize'] = titanic_master['SibSp'] + titanic_master['Parch']\n",
    "test_data['Famsize'] = test_data['SibSp'] + test_data['Parch']\n",
    "\n",
    "#family size, including the individual\n",
    "titanic_master['Count_Individ'] = titanic_master['Famsize'] + 1\n",
    "test_data['Count_Individ'] = test_data['Famsize'] + 1\n",
    "\n",
    "#name length \n",
    "titanic_master['Namelen'] = titanic_master['Name'].apply(lambda x: len(x))\n",
    "test_data['Namelen'] = test_data['Name'].apply(lambda x: len(x))\n",
    "\n",
    "#age groups\n",
    "titanic_master['Agecat']= titanic_master['Age']\n",
    "titanic_master.loc[(titanic_master.Age <= 10), 'Agecat'] = 1 #child\n",
    "titanic_master.loc[(titanic_master.Age > 60), 'Agecat'] = 4 #senior\n",
    "titanic_master.loc[(titanic_master.Age > 10) & (titanic_master.Age <= 30), 'Agecat'] = 2 #young adult\n",
    "titanic_master.loc[(titanic_master.Age > 30) & (titanic_master.Age <= 60), 'Agecat'] = 3 #adult\n",
    "test_data['Agecat']= test_data['Age']\n",
    "test_data.loc[(test_data.Age <= 10), 'Agecat'] = 1\n",
    "test_data.loc[(test_data.Age > 60), 'Agecat'] = 4\n",
    "test_data.loc[(test_data.Age > 10) & (test_data.Age <= 30), 'Agecat'] = 2\n",
    "test_data.loc[(test_data.Age > 30) & (test_data.Age <= 60), 'Agecat'] = 3\n",
    "\n",
    "#cabin indicator (as seen in blog post)\n",
    "titanic_master.loc[titanic_master.Cabin.isnull()==True,'Cabin'] = 0.5\n",
    "titanic_master.loc[titanic_master.Cabin.isnull()==False,'Cabin'] = 1.\n",
    "test_data.loc[test_data.Cabin.isnull()==True,'Cabin'] = 0.5\n",
    "test_data.loc[test_data.Cabin.isnull()==False,'Cabin'] = 1.\n",
    "\n",
    "#wealth and sex\n",
    "titanic_master['ClassSex'] = titanic_master['Sex'] * titanic_master['Pclass']\n",
    "test_data['ClassSex'] = test_data['Sex'] * titanic_master['Pclass']\n",
    "\n",
    "#age and sex\n",
    "titanic_master['AgeSex'] = titanic_master['Age'] * titanic_master['Sex']\n",
    "test_data['AgeSex'] = test_data['Age'] * test_data['Sex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#title\n",
    "def get_title(name):\n",
    "    title_search = re.search('([A-Za-z]+)\\.', name)\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return \"\"\n",
    "\n",
    "def replace_titles(x):\n",
    "    title = x['Title']\n",
    "    if title in ['Mr', 'don', 'Major', 'Capt', 'Jonkheer', 'Rev', 'Col']:\n",
    "        return 'Mr'\n",
    "    elif title in ['Master']:\n",
    "        return 'Master'\n",
    "    elif title in ['Mlle', 'Ms', 'Miss', 'Lady']:\n",
    "        return 'Miss'\n",
    "    elif title in ['Countess', 'Mme', 'Mrs', 'Dona']:\n",
    "        return 'Mrs'\n",
    "    elif title == 'Dr':\n",
    "        if x['Sex'] == 0:\n",
    "            return 'Mr'\n",
    "        else:\n",
    "            return 'Mrs'\n",
    "    elif title == '':\n",
    "        if x['Sex'] == 0:\n",
    "            return 'Master'\n",
    "        else:\n",
    "            return 'Miss'\n",
    "    else:\n",
    "        return title\n",
    "\n",
    "\n",
    "titles = titanic_master['Name'].apply(get_title)\n",
    "title_mapping = {'Mr': 1, 'Miss': 2, 'Mrs': 3, 'Master': 4, 'Dr': 5, 'Rev': 6, 'Major': 7, 'Col': 7, 'Mlle': 8, 'Mme': 8, 'Don': 9, 'Lady': 10, 'Countess': 10, 'Jonkheer': 10, 'Sir': 9, 'Capt': 7, 'Ms': 2, 'Dona':10}\n",
    "for k,v in title_mapping.items():\n",
    "    titles[titles == k] = v\n",
    "titanic_master['Title'] = titles    \n",
    "titanic_master['Title'] = titanic_master.apply(replace_titles, axis=1)\n",
    "\n",
    "test_titles = test_data['Name'].apply(get_title)\n",
    "title_mapping = {'Mr': 1, 'Miss': 2, 'Mrs': 3, 'Master': 4, 'Dr': 5, 'Rev': 6, 'Major': 7, 'Col': 7, 'Mlle': 8, 'Mme': 8, 'Don': 9, 'Lady': 10, 'Countess': 10, 'Jonkheer': 10, 'Sir': 9, 'Capt': 7, 'Ms': 2, 'Dona':10}\n",
    "for k,v in title_mapping.items():\n",
    "    test_titles[test_titles == k] = v\n",
    "test_data['Title'] = test_titles\n",
    "test_data['Title'] = test_data.apply(replace_titles, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#family id\n",
    "family_id_mapping = {}\n",
    "def get_family_id(row):\n",
    "    last_name = row['Name'].split(',')[0]\n",
    "    family_id = '{0}{1}'.format(last_name, row['Famsize'])\n",
    "    if family_id not in family_id_mapping:\n",
    "        if len(family_id_mapping) == 0:\n",
    "            current_id = 1\n",
    "        else:\n",
    "            current_id = (max(family_id_mapping.items(), key=operator.itemgetter(1))[1] + 1)\n",
    "        family_id_mapping[family_id]=current_id\n",
    "    return family_id_mapping[family_id]\n",
    "\n",
    "family_ids = titanic_master.apply(get_family_id, axis=1)\n",
    "family_ids[titanic_master['Famsize'] < 3] = -1\n",
    "titanic_master['Familyid'] = family_ids\n",
    "\n",
    "family_ids = test_data.apply(get_family_id, axis=1)\n",
    "family_ids[test_data['Famsize'] < 3] = -1\n",
    "test_data['Familyid'] = family_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that a bunch of indicators have been added, I want to look over the signal coming from these indicators, and start thinking about the relationships between the strong indicators. I use the approach described in the Dataquest tutorial in order to achieve this. I then apply the model to the testing data in order to see whether there was improvement using this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAE6CAYAAAALL9kIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHZdEHhHRcUnkEREd97+ScsbEZNrprQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbb54a10c10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.81593714927\n"
     ]
    }
   ],
   "source": [
    "predictors = ['Pclass','Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked','Famsize', 'Title', 'Familyid', 'Count_Individ', 'AgeSex', 'ClassSex', 'Agecat', 'Cabin']\n",
    "selector = SelectKBest(f_classif, k=5)\n",
    "selector.fit(titanic_master[predictors], titanic_master['Survived'])\n",
    "scores = -np.log10(selector.pvalues_)\n",
    "\n",
    "plt.bar(range(len(predictors)), scores)\n",
    "plt.xticks(range(len(predictors)), predictors, rotation='vertical')\n",
    "plt.show()\n",
    "\n",
    "best_predictors = ['Pclass', 'Sex', 'Fare', 'Title', 'AgeSex', 'ClassSex']\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=8, min_samples_leaf=4)\n",
    "kf = cross_validation.KFold(titanic_master.shape[0], n_folds=3, random_state=1)\n",
    "scores = cross_validation.cross_val_score(alg,titanic_master[best_predictors],titanic_master['Survived'],cv=3)\n",
    "print scores.mean() \n",
    "\n",
    "#apply to testing data, use best_predictors\n",
    "alg.fit(titanic_master[best_predictors], titanic_master['Survived'])\n",
    "predictions = alg.predict_proba(test_data[best_predictors].astype(float))[:,1]\n",
    "\n",
    "predictions[predictions <= 0.5] = 0\n",
    "predictions[predictions > 0.5] = 1\n",
    "\n",
    "final_pred = []\n",
    "for element in predictions:\n",
    "    final_pred.append(int(element))\n",
    "\n",
    "submission = pd.DataFrame({'PassengerId': test_data['PassengerId'], 'Survived':final_pred})\n",
    "submission.to_csv('kaggle_v2_randforestimproved.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with this improved random forest, my score still sits at 0.76077. This implies to me that I've distilled some of the fundamental signal using this method, but there is a chance that this method has overfit the data by some amount. Taking a cue from the Dataquest model, I next explore ensembling and gradient boosting in order to pull together different methodologies in order to get a stronger correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.817059483726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vpreston/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:19: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n"
     ]
    }
   ],
   "source": [
    "#I'll try gradient boosting and a logistic regression in this first pass, with all of the predictors\n",
    "algos = [[GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), predictors],[LogisticRegression(random_state=1), predictors]]\n",
    "kf = cross_validation.KFold(titanic_master.shape[0], n_folds=3, random_state=1)\n",
    "\n",
    "preds = []\n",
    "for train, test in kf:\n",
    "    train_tar = titanic_master['Survived'].iloc[train]\n",
    "    full_test_pred = []\n",
    "    for alg, predictors in algos:\n",
    "        alg.fit(titanic_master[predictors].iloc[train,:], train_tar)\n",
    "        test_preds = alg.predict_proba(titanic_master[predictors].iloc[test,:].astype(float))[:,1]\n",
    "        full_test_pred.append(test_preds)\n",
    "    test_preds = (full_test_pred[0]*2 + full_test_pred[1])/3 #this weighs gradient heavily equally\n",
    "    test_preds[test_preds <= 0.5] = 0\n",
    "    test_preds[test_preds >0.5] = 1\n",
    "    preds.append(test_preds)\n",
    "\n",
    "predictions = np.concatenate(preds, axis=0)\n",
    "accuracy = sum(predictions[predictions == titanic_master['Survived']])/len(predictions)\n",
    "print accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for a final submission\n",
    "final_predictions = []\n",
    "for alg, predictors in algos:\n",
    "    alg.fit(titanic_master[predictors], titanic_master['Survived'])\n",
    "    predictions = alg.predict_proba(test_data[predictors].astype(float))[:,1]\n",
    "    final_predictions.append(predictions)\n",
    "\n",
    "predictions = (final_predictions[0] * 2 + final_predictions[1]) / 3\n",
    "predictions[predictions <= 0.5] = 0\n",
    "predictions[predictions > 0.5] = 1\n",
    "\n",
    "final_pred = []\n",
    "for element in predictions:\n",
    "    final_pred.append(int(element))\n",
    "\n",
    "submission = pd.DataFrame({'PassengerId': test_data['PassengerId'], 'Survived':final_pred})\n",
    "submission.to_csv('kaggle_v2_ensemble1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ensemble brought my score up to 0.78947, which is only a small amount better than my first iteration model score using a simple formula. I now explore improvements to this model, starting with the predictors (namely, narrowing them down to known quantities with signal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.822671156004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vpreston/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:21: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n"
     ]
    }
   ],
   "source": [
    "grad_pred = ['Pclass','Sex', 'Age', 'Fare', 'Embarked', 'Title', 'Familyid', 'Famsize', 'ClassSex', 'AgeSex']\n",
    "log_pred = ['Pclass', 'Sex', 'Fare', 'Title', 'Famsize', 'Age', 'Embarked', 'ClassSex', 'AgeSex']\n",
    "\n",
    "algos = [[GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), grad_pred],[LogisticRegression(random_state=1), log_pred]]\n",
    "kf = cross_validation.KFold(titanic_master.shape[0], n_folds=3, random_state=1)\n",
    "\n",
    "preds = []\n",
    "for train, test in kf:\n",
    "    train_tar = titanic_master['Survived'].iloc[train]\n",
    "    full_test_pred = []\n",
    "    for alg, predictors in algos:\n",
    "        alg.fit(titanic_master[predictors].iloc[train,:], train_tar)\n",
    "        test_preds = alg.predict_proba(titanic_master[predictors].iloc[test,:].astype(float))[:,1]\n",
    "        full_test_pred.append(test_preds)\n",
    "    test_preds = (full_test_pred[0] * 3 + full_test_pred[1]) / 4\n",
    "    test_preds[test_preds <= 0.5] = 0\n",
    "    test_preds[test_preds >0.5] = 1\n",
    "    preds.append(test_preds)\n",
    "\n",
    "predictions = np.concatenate(preds, axis=0)\n",
    "accuracy = sum(predictions[predictions == titanic_master['Survived']])/len(predictions)\n",
    "print accuracy\n",
    "\n",
    "#for test data\n",
    "final_predictions = []\n",
    "for alg, predictors in algos:\n",
    "    alg.fit(titanic_master[predictors], titanic_master['Survived'])\n",
    "    predictions = alg.predict_proba(test_data[predictors].astype(float))[:,1]\n",
    "    final_predictions.append(predictions)\n",
    "\n",
    "predictions = (final_predictions[0] * 3 + final_predictions[1]) / 4\n",
    "predictions[predictions <= 0.5] = 0\n",
    "predictions[predictions > 0.5] = 1\n",
    "\n",
    "final_pred = []\n",
    "for element in predictions:\n",
    "    final_pred.append(int(element))\n",
    "\n",
    "submission = pd.DataFrame({'PassengerId': test_data['PassengerId'], 'Survived':final_pred})\n",
    "submission.to_csv('kaggle_v2_ensembleimproved.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This indicator adjustment actually got me to the \"advertised\" accuracy of 0.79904 from the Dataquest experience. Now, I want to explore the relationships I had begun to explore in my first exploration, but now with more advanced categories of data. I draw upon inspiration from the blog post mentioned earlier, however the formula is my own and is based upon the data mining technique covered in ThinkStats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.409502\n",
      "         Iterations 8\n",
      "0.83164983165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vpreston/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:11: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "formula = 'Survived~ClassSex*Age+Fare+Title+Famsize*Pclass+Embarked*Agecat'\n",
    "model = smf.logit(formula, data=titanic_master)\n",
    "results = model.fit()\n",
    "\n",
    "#assess our training\n",
    "pred = results.predict()\n",
    "pred[pred >= 0.5] = 1\n",
    "pred[pred < 0.5] = 0\n",
    "accuracy = sum(pred[pred == titanic_master['Survived']] / len(pred))\n",
    "print accuracy\n",
    "\n",
    "new = test_data\n",
    "predicts = results.predict(new)\n",
    "\n",
    "predicts[predicts > 0.5] = int(1)\n",
    "predicts[predicts <= 0.5] = int(0)\n",
    "\n",
    "final_pred = []\n",
    "for element in predicts:\n",
    "    final_pred.append(int(element))\n",
    "\n",
    "#now make a submission dataframe for Kaggle\n",
    "submission = pd.DataFrame({'PassengerId': test_data['PassengerId'], 'Survived':final_pred})\n",
    "# print submission\n",
    "submission.to_csv('kaggle_v2_formula.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This self-defined model achieved me a score just slightly less than that of the dataquest version, 0.79900. I'm excited that I've seemed to hone in on some of the most powerful relationships using intuition and the \"goodness\" of the predictors score as achieved earlier. I'm going to define new categories based on some of these powerful relationships and then apply them to an ensembled approach which has given me my best scores yet. I'm also going to make some fare and family relationship joint indicators, as inspired by the blog I've mentioned, as well as try out some decision tree work inspired by [this](https://www.kaggle.com/c/titanic/forums/t/6821/titanic-getting-started-with-r-full-guide-to-0-81340) resource Paul cited in the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.811447811448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vpreston/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:18: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "predictors = best_predictors\n",
    "\n",
    "regr = DecisionTreeRegressor(max_depth=len(predictors))\n",
    "kf = cross_validation.KFold(titanic_master.shape[0], n_folds=3, random_state=1)\n",
    "\n",
    "preds = []\n",
    "for train, test in kf:\n",
    "    train_tar = titanic_master['Survived'].iloc[train]\n",
    "    regr.fit(titanic_master[predictors].iloc[train,:], train_tar)\n",
    "    test_preds = regr.predict(titanic_master[predictors].iloc[test,:].astype(float))\n",
    "    test_preds[test_preds >= 0.5] = 1\n",
    "    test_preds[test_preds < 0.5] = 0\n",
    "    preds.append(test_preds[0:])\n",
    "\n",
    "predictions = np.concatenate(preds,axis=0)\n",
    "accuracy = sum(predictions[predictions == titanic_master['Survived']])/len(predictions)\n",
    "print accuracy\n",
    "\n",
    "#on test data\n",
    "regr.fit(titanic_master[predictors], titanic_master['Survived'])\n",
    "predicts = regr.predict(test_data[predictors])\n",
    "\n",
    "predicts[predicts > 0.5] = int(1)\n",
    "predicts[predicts <= 0.5] = int(0)\n",
    "\n",
    "final_pred = []\n",
    "for element in predicts:\n",
    "    final_pred.append(int(element))\n",
    "\n",
    "#now make a submission dataframe for Kaggle\n",
    "submission = pd.DataFrame({'PassengerId': test_data['PassengerId'], 'Survived':final_pred})\n",
    "\n",
    "# print submission\n",
    "submission.to_csv('kaggle_v2_decisiontree.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This decision tree implementation recieved a scores of 0.77512. Not bad, but also not an improvement over the Dataquest model. The blog further suggests applying a random forest to the decision trees. Instead, I'm going to try an ensemble of a gradient, logistic, and decision tree in my last attempt at betterng the Dataquest model. This is largely driven by the idea that the decision tree gives some flexibility in a model which performs relatively well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.827160493827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vpreston/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:21: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n"
     ]
    }
   ],
   "source": [
    "dtr_pred = best_predictors\n",
    "grad_pred = ['Pclass','Sex', 'Age', 'Fare', 'Embarked', 'Title', 'Familyid', 'Famsize', 'ClassSex', 'AgeSex']\n",
    "log_pred = ['Pclass', 'Sex', 'Fare', 'Title', 'Famsize', 'Age', 'Embarked', 'ClassSex', 'AgeSex']\n",
    "algos = [[DecisionTreeRegressor(max_depth=3),dtr_pred], [GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), grad_pred],[LogisticRegression(random_state=1), log_pred]]\n",
    "kf = cross_validation.KFold(titanic_master.shape[0], n_folds=3, random_state=1)\n",
    "\n",
    "preds = []\n",
    "for train, test in kf:\n",
    "    train_tar = titanic_master['Survived'].iloc[train]\n",
    "    full_test_pred = []\n",
    "    for alg, predictors in algos:\n",
    "        alg.fit(titanic_master[predictors].iloc[train,:], train_tar)\n",
    "        test_preds = alg.predict(titanic_master[predictors].iloc[test,:].astype(float))\n",
    "        full_test_pred.append(test_preds)\n",
    "    test_preds = (full_test_pred[0]*2 + full_test_pred[1] + full_test_pred[2]) / 4\n",
    "    test_preds[test_preds <= 0.5] = 0\n",
    "    test_preds[test_preds >0.5] = 1\n",
    "    preds.append(test_preds)\n",
    "\n",
    "predictions = np.concatenate(preds,axis=0)\n",
    "accuracy = sum(predictions[predictions == titanic_master['Survived']])/len(predictions)\n",
    "print accuracy\n",
    "\n",
    "#on test data\n",
    "final_predictions = []\n",
    "for alg, predictors in algos:\n",
    "    alg.fit(titanic_master[predictors], titanic_master['Survived'])\n",
    "    predictions = alg.predict(test_data[predictors].astype(float))\n",
    "    final_predictions.append(predictions)\n",
    "\n",
    "predictions = (final_predictions[0] * 2 + final_predictions[1] + final_predictions[2]) / 4\n",
    "predictions[predictions <= 0.5] = 0\n",
    "predictions[predictions > 0.5] = 1\n",
    "\n",
    "final_pred = []\n",
    "for element in predictions:\n",
    "    final_pred.append(int(element))\n",
    "\n",
    "#now make a submission dataframe for Kaggle\n",
    "submission = pd.DataFrame({'PassengerId': test_data['PassengerId'], 'Survived':final_pred})\n",
    "\n",
    "# print submission\n",
    "submission.to_csv('kaggle_v2_decisiontreeensemble.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This final submission bought me a score of 0.80383. This implies to me that the addition of the decision tree to my previous ensemble was valuable. My theory on this matter is that the decision tree offers some amount of implicit weighting to individuals based upon the factors given. This both allows for more variance in the accuracy (which I consider to be good in this case because at about 0.799 it is really only a few values which need to be flipped) and also means that overfitting to the data can be easy. To avoid the overfitting data, I aimed to keep my fold accuracy at about the point which seems to yield the best results on the testing data (just over 80%). More than this number seems to get me data that matches well with the training but poorly with the testing, whereas less gets me...less. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the conclusion of this warmup project, I feel that I've had a chance to practice and read more on data cleaning, data mining, formulaic approaches to fitting data, and built-in data fitting packages in scikitlearn. I am also left with a few questions: how does one detext overfitting? is there a way to predict testing accuracy without crossreferncing the answers? what ethodical approach to data scientists typically take to finding and fitting a model to data? how much of data science is being clever (not necessarily being analytical)? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I enjoyed in this last iteration the ability to explore the concept of ensembles more, and to apply a new technique listed in a class resource on decision trees. The resource expounded on the concept using a programming language R which I abstracted using scikitlearn. I would be curious to see how similar the underlying algorithms are for these two fitting techniques. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
