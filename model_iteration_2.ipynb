{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the DataQuest Kaggle module \"Improving your submission\" as a resource in my second model iteration.  The first thing it did was introduce the random forest model, so I will start by creating one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import re\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanData(data, median_age):\n",
    "    \"\"\"\n",
    "    Take in the raw data and median age from the training data\n",
    "    and return a cleaned version for use with our model\n",
    "    \"\"\"\n",
    "    # Replace missing ages with the median age (from the training data!)\n",
    "    data.Age = data.Age.fillna(median_age)\n",
    "\n",
    "    # Encode male as 0 and female as 1\n",
    "    data.loc[data.Sex == 'male', 'Sex'] = 0\n",
    "    data.loc[data.Sex == 'female', 'Sex'] = 1\n",
    "\n",
    "    # Replace missing port of embarkation with Southampton\n",
    "    # Emcode Southampton as 0, Cherbourg as 1, and Queenstown as 2\n",
    "    data.Embarked = data.Embarked.fillna('S')\n",
    "    data.loc[data.Embarked == 'S', 'Embarked'] = 0\n",
    "    data.loc[data.Embarked == 'C', 'Embarked'] = 1\n",
    "    data.loc[data.Embarked == 'Q', 'Embarked'] = 2\n",
    "    \n",
    "    # Replace missing fares with the median fare\n",
    "    data.Fare = data.Fare.fillna(data.Fare.median())\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic = pd.read_csv('train.csv')\n",
    "titanic_test = pd.read_csv('test.csv')\n",
    "\n",
    "titanic = cleanData(titanic, titanic.Age.median())\n",
    "titanic_test = cleanData(titanic_test, titanic.Age.median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.022% accuracy\n"
     ]
    }
   ],
   "source": [
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=10, min_samples_split=2, min_samples_leaf=1)\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic.Survived, cv=3)\n",
    "print '{:.3f}% accuracy'.format(scores.mean()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already it's looking like a random forest model is doing quite a bit better than the logistic regression from iteration 1!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweak model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.930% accuracy\n"
     ]
    }
   ],
   "source": [
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=4, min_samples_leaf=2)\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic.Survived, cv=3)\n",
    "print '{:.3f}% accuracy'.format(scores.mean()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the number of trees used, number of samples required to split, and number of samples required to create a leaf improved the cross validation accuracy by nearly 2% and will help avoid overfitting to the training data.  Let's see how this model does with the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def testModel(alg, train, test, predictors):\n",
    "    \"\"\"\n",
    "    Take in the training data, testing data, and predictors to use\n",
    "    and return the submission dataframe for a logistic regression model\n",
    "    \"\"\"\n",
    "    # Train the algorithm using all the training data\n",
    "    alg.fit(train[predictors], train.Survived)\n",
    "\n",
    "    # Make predictions using the test set.\n",
    "    predictions = alg.predict(test[predictors])\n",
    "\n",
    "    # Create a new dataframe with only the columns Kaggle wants from the dataset.\n",
    "    return pd.DataFrame({\n",
    "        'PassengerId': test.PassengerId,\n",
    "        'Survived': predictions\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=4, min_samples_leaf=2)\n",
    "submission = testModel(alg, titanic, titanic_test, predictors)\n",
    "\n",
    "# Write submission to a CSV file\n",
    "submission.to_csv('submission_4.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "75.120 accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the cross validation score was higher than any from the logistic regression model, the random forest model performed the same on the test data as our very first submission.  It seems to me that random forests may be more susceptible to overfitting than the logistic regression, so I will have to be aware that a better cross validation score may not mean that a model will be more accurate with the test data.  I'll deviate from the DataQuest module and try increasing `min_samples_split` and `min_samples_leaf` a bit more to see if that helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.706% accuracy\n"
     ]
    }
   ],
   "source": [
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=8, min_samples_leaf=4)\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic.Survived, cv=3)\n",
    "print '{:.3f}% accuracy'.format(scores.mean()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I expected, the cross validation score was a bit lower.  This may indicate that the model has less overfitting and will perform better with the test data.  Let's check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=8, min_samples_leaf=4)\n",
    "submission = testModel(alg, titanic, titanic_test, predictors)\n",
    "\n",
    "# Write submission to a CSV file\n",
    "submission.to_csv('submission_5.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "77.990% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is the most accurate so far!  I was correct in thinking that this model would be less overfit to the training data and thus do better with the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing the DataQuest module did was create new features.  I'll follow along, create some of my own, then evaluate how much of an impact these features have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTitle(name):\n",
    "    \"\"\"\n",
    "    Take in a name and return the title encoded as an integer, if there is one\n",
    "    \"\"\"\n",
    "    # Use a regular expression to search for a title.\n",
    "    # Titles always consist of capital and lowercase letters, and end with a period.\n",
    "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "    # If the title exists, extract and encode it.\n",
    "    if title_search:\n",
    "        title = title_search.group(1)\n",
    "\n",
    "        # Map each title to an integer.\n",
    "        # Some titles are very rare, and are compressed into the same codes as other titles.\n",
    "        title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Dr\": 5, \"Rev\": 6,\n",
    "                         \"Major\": 7, \"Col\": 7, \"Mlle\": 8, \"Mme\": 8, \"Don\": 9, \"Lady\": 10,\n",
    "                         \"Countess\": 10, \"Jonkheer\": 10, \"Sir\": 9, \"Capt\": 7, \"Ms\": 2}\n",
    "        try:\n",
    "            return title_mapping[title]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return 0\n",
    "\n",
    "def getFamilyId(row, family_id_mapping):\n",
    "    \"\"\"\n",
    "    Take in a row and the family id map and return the family id and updated map\n",
    "    Must be run after FamilySize column is created\n",
    "    \"\"\"\n",
    "    # Find the last name by splitting on a comma\n",
    "    last_name = row.Name.split(\",\")[0]\n",
    "    # Create the family id\n",
    "    family_id = '{}{}'.format(last_name, row.FamilySize)\n",
    "    # Look up the id in the mapping\n",
    "    if family_id not in family_id_mapping:\n",
    "        if len(family_id_mapping) == 0:\n",
    "            current_id = 1\n",
    "        else:\n",
    "            # Get the maximum id from the mapping and add one to it if we don't have an id\n",
    "            current_id = (max(family_id_mapping.items(), key=operator.itemgetter(1))[1] + 1)\n",
    "        family_id_mapping[family_id] = current_id\n",
    "    return family_id_mapping[family_id], family_id_mapping\n",
    "\n",
    "def createDQFeatures(data, family_id_mapping):\n",
    "    \"\"\"\n",
    "    Add the features from the DataQuest module\n",
    "    \"\"\"\n",
    "    # Generating a familysize column\n",
    "    data[\"FamilySize\"] = data.SibSp + data.Parch\n",
    "\n",
    "    # The .apply method generates a new series\n",
    "    data[\"NameLength\"] = data.Name.apply(lambda x: len(x))\n",
    "\n",
    "    # Add a title column\n",
    "    data[\"Title\"] = data.Name.apply(getTitle)\n",
    "\n",
    "    # Add a family id column\n",
    "    family_ids = pd.Series(np.zeros((len(data),)))\n",
    "    # Unlike in DataQuest, I don't want to use global variables\n",
    "    # Unfortunately, this means I have to iterate over the rows manually\n",
    "    for i, row in data.iterrows():\n",
    "        family_id, family_id_mapping = getFamilyId(row, family_id_mapping)\n",
    "        family_ids[i] = family_id\n",
    "    # There are a lot of family ids, so we'll compress all of the families under 3 members into one code.\n",
    "    family_ids[titanic.FamilySize < 3] = -1\n",
    "    data[\"FamilyId\"] = family_ids\n",
    "\n",
    "    return data, family_id_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We want to use the same family ID map in the training and test data, so we need to keep\n",
    "# track of it (and I don't want to use global variables like the DataQuest module does)\n",
    "family_id_mapping = {}\n",
    "titanic, family_id_mapping = createDQFeatures(titanic, family_id_mapping)\n",
    "titanic_test, family_id_mapping = createDQFeatures(titanic_test, family_id_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getTitle2(name):\n",
    "    \"\"\"\n",
    "    Take in a name and return the title encoded as an integer, if there is one\n",
    "    \"\"\"\n",
    "    # Use a regular expression to search for a title.\n",
    "    # Titles always consist of capital and lowercase letters, and end with a period.\n",
    "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "    # If the title exists, extract and encode it.\n",
    "    if title_search:\n",
    "        title = title_search.group(1)\n",
    "\n",
    "        # Map each title to an integer. (Revised groupings by me)\n",
    "        # 1: Mr (adult male)\n",
    "        # 2: Miss, Mlle, Ms (young female)\n",
    "        # 3: Mrs, Mme (adult female)\n",
    "        # 4: Master (young male)\n",
    "        # 5: Dr, Rev (special adult)\n",
    "        # 6: Major, Col (military)\n",
    "        # 7: Don, Jonkheer, Sir, Capt (very special male)\n",
    "        # 8: Lady, Countess (very special female)\n",
    "        title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Dr\": 5, \"Rev\": 5,\n",
    "                         \"Major\": 6, \"Col\": 6, \"Mlle\": 2, \"Mme\": 3, \"Don\": 7, \"Lady\": 8,\n",
    "                         \"Countess\": 8, \"Jonkheer\": 7, \"Sir\": 7, \"Capt\": 7, \"Ms\": 2}\n",
    "        try:\n",
    "            return title_mapping[title]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return 0\n",
    "\n",
    "def createMyFeatures(data):\n",
    "    \"\"\"\n",
    "    Add my features\n",
    "    \"\"\"\n",
    "    # Add a flag for children under the age of 9\n",
    "    data[\"IsChild\"] = data.Age.apply(lambda x: 1 if x < 9 else 0)\n",
    "    \n",
    "    # Add another family size column that includes the passenger (like we talked about in class)\n",
    "    data[\"FamilySize2\"] = data.SibSp + data.Parch + 1\n",
    "    \n",
    "    # Add another title column that groups the titles differently\n",
    "    data[\"Title2\"] = data.Name.apply(getTitle2)\n",
    "\n",
    "    return data\n",
    "\n",
    "titanic = createMyFeatures(titanic)\n",
    "titanic_test = createMyFeatures(titanic_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select best features to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68.851994252857665, 'Sex')\n",
      "(32.002333704918456, 'Title2')\n",
      "(26.983386072106185, 'Title')\n",
      "(24.595671420777524, 'Pclass')\n",
      "(23.693190161546514, 'NameLength')\n",
      "(14.213235141762933, 'Fare')\n",
      "(5.0206988771330954, 'IsChild')\n",
      "(2.8513009904508668, 'Embarked')\n",
      "(1.8716004089590674, 'FamilyId')\n",
      "(1.8297604290610845, 'Parch')\n",
      "(1.2776895459668496, 'Age')\n",
      "(0.53425450244248629, 'SibSp')\n",
      "(0.20768458341872537, 'FamilySize2')\n",
      "(0.20768458341872537, 'FamilySize')\n"
     ]
    }
   ],
   "source": [
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\",\n",
    "              \"FamilySize\", \"NameLength\", \"Title\", \"FamilyId\", \n",
    "              \"IsChild\", \"FamilySize2\", \"Title2\"]\n",
    "\n",
    "# Perform feature selection\n",
    "selector = SelectKBest(f_classif, k='all')\n",
    "selector.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "\n",
    "# Get the raw p-values for each feature, and transform from p-values into scores\n",
    "scores = -np.log10(selector.pvalues_)\n",
    "\n",
    "for x in sorted(zip(scores, predictors))[::-1]:\n",
    "    print x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the DataQuest module, they didn't include name length in this part, so I was surprised to see how high it ranked among the predictors.  Maybe the `f_classif` function rated it highly, but it doesn't do much for the random forest.  After seeing some of the data from the name column, it would seem to me that name length would not be that useful for indicating wealth which is what the DataQuest module intended when creating that column.  I was also surprised that title was the only other new feature in the top five.  It looks like my encodings of the titles were slightly better than the ones from the DataQuest module, but including the passenger in the family size had no effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I want to see how adding predictors one-by-one impacts cross validation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 78.676% accuracy\n",
      "2: 79.349% accuracy\n",
      "3: 77.778% accuracy\n",
      "4: 81.369% accuracy\n",
      "5: 80.808% accuracy\n",
      "6: 82.267% accuracy\n",
      "7: 82.492% accuracy\n",
      "8: 82.492% accuracy\n"
     ]
    }
   ],
   "source": [
    "# Predictors in order of importance, leaving out NameLength and using Title2 instead of Title\n",
    "predictors = [\"Sex\", \"Title2\", \"Pclass\", \"Fare\", \"IsChild\", \"Embarked\", \"FamilyId\", \"FamilySize\"]\n",
    "\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=8, min_samples_leaf=4)\n",
    "for i in range(len(predictors)):\n",
    "    scores = cross_validation.cross_val_score(alg, titanic[predictors[:i+1]], titanic.Survived, cv=3)\n",
    "    print '{}: {:.3f}% accuracy'.format(i+1, scores.mean()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of times when adding more predictors reduces accuracy, which is surprising to me.  I'll try a few different sets of predictors to see how they do with the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictors = [\"Sex\", \"Title2\", \"Pclass\", \"Fare\"]\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=8, min_samples_leaf=4)\n",
    "submission = testModel(alg, titanic, titanic_test, predictors)\n",
    "\n",
    "# Write submission to a CSV file\n",
    "submission.to_csv('submission_6.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "77.033% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using only the top 4 predictors performed worse than the previous submission that used none of the added features.  This isn't too surprising since it had a slighly lower cross validation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictors = [\"Sex\", \"Title2\", \"Pclass\", \"Fare\", \"IsChild\", \"Embarked\"]\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=8, min_samples_leaf=4)\n",
    "submission = testModel(alg, titanic, titanic_test, predictors)\n",
    "\n",
    "# Write submission to a CSV file\n",
    "submission.to_csv('submission_7.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "75.598% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the next 2 predictors decreased the accuracy by over 1%.  Judging by the cross validation scores, this model should have done better.  This may be an indication that the model is overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictors = [\"Sex\", \"Title2\", \"Pclass\", \"Fare\", \"IsChild\", \"Embarked\", \"FamilyId\"]\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=8, min_samples_leaf=4)\n",
    "submission = testModel(alg, titanic, titanic_test, predictors)\n",
    "\n",
    "# Write submission to a CSV file\n",
    "submission.to_csv('submission_8.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "76.077% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the family ID improved performance slightly, but it is still worse than just using the top 4 predictors.  It seems like using fewer predictors helps prevent overfitting and allows the model to perform better with the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, none of the models that use the new features have improved with the test data.  So, I want to see how a model does that _only_ uses the new features (except for name length)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.267% accuracy\n"
     ]
    }
   ],
   "source": [
    "predictors = [\"Title2\", \"IsChild\", \"FamilyId\", \"FamilySize\"]\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=8, min_samples_leaf=4)\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic.Survived, cv=3)\n",
    "print '{:.3f}% accuracy'.format(scores.mean()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors = [\"Title2\", \"IsChild\", \"FamilyId\", \"FamilySize\"]\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=8, min_samples_leaf=4)\n",
    "submission = testModel(alg, titanic, titanic_test, predictors)\n",
    "\n",
    "# Write submission to a CSV file\n",
    "submission.to_csv('submission_9.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "78.469% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This improved our best performance!  It is interesting that the top 4 predictors according to `SelectKBest` and `f_classif` were not as good as the 4 added predictors.  It seems that using a smaller number of predictors is good for preventing overfitting and `SelectKBest` is not necessarily good at selecting the best predictors.  I'm going to choose a set of predictors based on my intuition while trying to keep the number low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.941% accuracy\n"
     ]
    }
   ],
   "source": [
    "predictors = [\"Sex\", \"Pclass\", \"Age\", \"Title2\", \"FamilyId\"]\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=8, min_samples_leaf=4)\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic.Survived, cv=3)\n",
    "print '{:.3f}% accuracy'.format(scores.mean()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictors = [\"Sex\", \"Pclass\", \"Age\", \"Title2\", \"FamilyId\"]\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=8, min_samples_leaf=4)\n",
    "submission = testModel(alg, titanic, titanic_test, predictors)\n",
    "\n",
    "# Write submission to a CSV file\n",
    "submission.to_csv('submission_10.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "74.163% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one did quite a bit worse, even though the cross validation score looked good.  I'm guessing that overfitting in the culprit again, prossibly due to using the family ID.  I wonder if replacing it with family size will help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.492% accuracy\n"
     ]
    }
   ],
   "source": [
    "predictors = [\"Sex\", \"Pclass\", \"Age\", \"Title2\", \"FamilySize\"]\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=8, min_samples_leaf=4)\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic.Survived, cv=3)\n",
    "print '{:.3f}% accuracy'.format(scores.mean()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictors = [\"Sex\", \"Pclass\", \"Age\", \"Title2\", \"FamilySize\"]\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=8, min_samples_leaf=4)\n",
    "submission = testModel(alg, titanic, titanic_test, predictors)\n",
    "\n",
    "# Write submission to a CSV file\n",
    "submission.to_csv('submission_11.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "74.641% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one also did worse than the model using only the added features.  I think it's time to try something other than just choosing different predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Gradient boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Gradient boosting involves training the decision trees one at a time and using the errors to help build the next tree.  Since this can easily lead to overfitting, the DataQuest module suggests limiting the number of trees to 25 and the tree depth to 3.  First, I'll use the predictors that the module uses (with my modified title groupings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.155% accuracy\n"
     ]
    }
   ],
   "source": [
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title2\", \"FamilyId\"]\n",
    "alg = GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3)\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic.Survived, cv=3)\n",
    "print '{:.3f}% accuracy'.format(scores.mean()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title2\", \"FamilyId\"]\n",
    "alg = GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3)\n",
    "submission = testModel(alg, titanic, titanic_test, predictors)\n",
    "\n",
    "# Write submission to a CSV file\n",
    "submission.to_csv('submission_12.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "80.383% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient boosting boosted our score to the highest yet!  This was also the smallest difference between cross validation score and test score so far, perhaps indicating that this model did not suffer from overfitting as much as the previous ones.  This is interesting to me since the DataQuest module used a lot of columns as predictors, so now I'm going to try using only the predictors from the best random forestâ€”the created features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.165% accuracy\n"
     ]
    }
   ],
   "source": [
    "predictors = [\"Title2\", \"IsChild\", \"FamilyId\", \"FamilySize\"]\n",
    "alg = GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3)\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic.Survived, cv=3)\n",
    "print '{:.3f}% accuracy'.format(scores.mean()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictors = [\"Title2\", \"IsChild\", \"FamilyId\", \"FamilySize\"]\n",
    "alg = GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3)\n",
    "submission = testModel(alg, titanic, titanic_test, predictors)\n",
    "\n",
    "# Write submission to a CSV file\n",
    "submission.to_csv('submission_13.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "77.512% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one didn't do as well, and had a large difference between cross validation score and test score.  I guess with this model, using a large number of predictors can work out pretty well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future explorations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing that the DataQuest module did was use both gradient boosting and a logistic regression as an ensemble.  However, their accuracy with the test data was slightly (1 or 2 predictions) lower than what I got with the gradient boosting alone, so I'm not sure if this would be worth pursuing.  With both the random forest and gradient boosting, I would be interested in looking into a better way of choosing which features to use as predictors.  `SelectKBest` didn't seem to do a great job, and my intuitions often did worse than I expected.  With the way Kaggle is set up, it's impossible to do an exhaustive search using the test data, and as we've seen in this iteration, higher cross validation scores do not necessarily mean better test accuracy.  Is there a good way to search across features to find the best ones to use as predictors?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
