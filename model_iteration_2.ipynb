{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "My model uses inspiration from the DataQuest modules, except that I got rid of family id as a predictor because I didn't think it was useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import useful libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import thinkstats2 as ts\n",
    "import thinkplot as tp\n",
    "import pylab as P\n",
    "import math\n",
    "import re\n",
    "import operator\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the preprocessing of the raw dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getParchAdj(n):\n",
    "    '''Groups people with >=2 parch together'''\n",
    "    if pd.isnull(n):\n",
    "        return n\n",
    "    if n == 0:\n",
    "        return 0\n",
    "    elif n == 1:\n",
    "        return 1\n",
    "    elif n > 1:\n",
    "        return 2\n",
    "    \n",
    "def getLog(x):\n",
    "    return math.log(x)\n",
    "\n",
    "def get_title(name):\n",
    "    # Use a regular expression to search for a title.  Titles always consist of capital and lowercase letters, and end with a period.\n",
    "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "    # If the title exists, extract and return it.\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return \"\"\n",
    "\n",
    "# # A function to get the id given a row\n",
    "# def get_family_id(row):\n",
    "#     # Find the last name by splitting on a comma\n",
    "#     last_name = row[\"Name\"].split(\",\")[0]\n",
    "#     # Create the family id\n",
    "#     family_id = \"{0}{1}\".format(last_name, row[\"FamilySize\"])\n",
    "#     # Look up the id in the mapping\n",
    "#     if family_id not in family_id_mapping:\n",
    "#         if len(family_id_mapping) == 0:\n",
    "#             current_id = 1\n",
    "#         else:\n",
    "#             # Get the maximum id from the mapping and add one to it if we don't have an id\n",
    "#             current_id = (max(family_id_mapping.items(), key=operator.itemgetter(1))[1] + 1)\n",
    "#         family_id_mapping[family_id] = current_id\n",
    "#     return family_id_mapping[family_id]\n",
    "    \n",
    "df = pd.read_csv('train.csv', header=0)\n",
    "\n",
    "def preprocess(dframe):\n",
    "    \n",
    "    frame = dframe.copy()\n",
    "    \n",
    "    # Fill NaN's with the medians\n",
    "    frame[\"Age\"] = frame[\"Age\"].fillna(df[\"Age\"].median())\n",
    "    frame[\"Fare\"] = frame[\"Fare\"].fillna(df[\"Fare\"].median())\n",
    "    frame[\"Embarked\"] = frame[\"Embarked\"].fillna(\"S\")\n",
    "    \n",
    "    # Numberize sex\n",
    "    frame.loc[frame[\"Sex\"] == \"male\", \"Sex\"] = 0 \n",
    "    frame.loc[frame[\"Sex\"] == \"female\", \"Sex\"] = 1\n",
    "    \n",
    "    # Numberize Embarked\n",
    "    frame.loc[frame[\"Embarked\"] == \"S\", \"Embarked\"] = 0\n",
    "    frame.loc[frame[\"Embarked\"] == \"C\", \"Embarked\"] = 1\n",
    "    frame.loc[frame[\"Embarked\"] == \"Q\", \"Embarked\"] = 2\n",
    "    \n",
    "    # Adjust the parch value\n",
    "    frame['Parchadj'] = frame.Parch.apply(getParchAdj)\n",
    "    \n",
    "    # get the log of Age\n",
    "    frame['AgeLog'] = frame.Age.apply(math.log)\n",
    "    \n",
    "    # Make a familysize column\n",
    "    frame['FamilySize'] = frame['SibSp'] + frame['Parch']\n",
    "    \n",
    "    frame[\"NameLength\"] = frame[\"Name\"].apply(lambda x: len(x))\n",
    "    \n",
    "    # Get all the titles\n",
    "    titles = frame[\"Name\"].apply(get_title)\n",
    "\n",
    "    # Map each title to an integer.  Some titles are very rare, and are compressed into the same codes as other titles.\n",
    "    title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Dr\": 5, \"Rev\": 6, \"Major\": 7, \"Col\": 7,\n",
    "                     \"Mlle\": 8, \"Mme\": 8, \"Don\": 9, \"Lady\": 10, \"Countess\": 10, \"Jonkheer\": 10, \"Sir\": 9, \"Capt\": 7, \"Ms\": 2, \"Dona\":10}\n",
    "    for k,v in title_mapping.items():\n",
    "        titles[titles == k] = v\n",
    "\n",
    "    # Add in the title column.\n",
    "    frame[\"Title\"] = titles\n",
    "    \n",
    "    return frame\n",
    "    \n",
    "\n",
    "df = preprocess(df)\n",
    "\n",
    "# The columns we'll use to predict the target\n",
    "# predictors = [\"Pclass\", \"Sex\"]\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I originally used \"Parchadj\" to categorize the Parch column.  It seemed like any number of parents and children above 2 had the same probability to survive as 2, so I grouped them all into a category of \"at least 2\".  I abandoned this column in favor of DataQuest's \"FamilySize\" column, which performed better and takes into account siblings as well.\n",
    "\n",
    "Originally, taking the natural logarithm of age and using that instead of actual age improved my scores.  I abandoned that approach because it did not improve my scores using the forest algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the accuracy of our model using Linear Regression and Random Forest Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.79797979798\n",
      "accuracy improvement: 0.0011223344558\n",
      "0.838383838384\n",
      "cv score improvement: 0.0415263748598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:27: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n"
     ]
    }
   ],
   "source": [
    "# Initialize our algorithm class\n",
    "alg = LinearRegression()\n",
    "# alg = RandomForestClassifier()\n",
    "# Generate cross validation folds for the titanic dataset.  It return the row indices corresponding to train and test.\n",
    "# We set random_state to ensure we get the same splits every time we run this.\n",
    "kf = KFold(df.shape[0], n_folds=3, random_state=1)\n",
    "\n",
    "predictions = []\n",
    "for train, test1 in kf:\n",
    "    # The predictors we're using the train the algorithm.  Note how we only take the rows in the train folds.\n",
    "    train_predictors = (df[predictors].iloc[train,:])\n",
    "    # The target we're using to train the algorithm.\n",
    "    train_target = df[\"Survived\"].iloc[train]\n",
    "    # Training the algorithm using the predictors and target.\n",
    "    alg.fit(train_predictors, train_target)\n",
    "    # We can now make predictions on the test fold\n",
    "    test_predictions = alg.predict(df[predictors].iloc[test1,:])\n",
    "    predictions.append(test_predictions)\n",
    "    \n",
    "# The predictions are in three separate numpy arrays.  Concatenate them into one.  \n",
    "# We concatenate them on axis 0, as they only have one axis.\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# Map predictions to outcomes (only possible outcomes are 1 and 0)\n",
    "predictions[predictions > .5] = 1\n",
    "predictions[predictions <=.5] = 0\n",
    "accuracy = sum(predictions[predictions == df[\"Survived\"]]) / len(predictions)\n",
    "print accuracy\n",
    "print 'accuracy improvement:', (accuracy - 0.796857463524)\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize our algorithm\n",
    "# alg = LogisticRegression(random_state=1)\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=8, min_samples_leaf=4)\n",
    "\n",
    "# Compute the accuracy score for all the cross validation folds.  (much simpler than what we did before!)\n",
    "scores = cross_validation.cross_val_score(alg, df[predictors], df[\"Survived\"], cv=3)\n",
    "# Take the mean of the scores (because we have one for each fold)\n",
    "print scores.mean()\n",
    "print 'cv score improvement:', (scores.mean() - 0.796857463524)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used comparison with a previous score to benchmark how well a new model would do compared to a previous model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the accuracy of our model using Gradient Boosting Classifier and Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.821548821549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:36: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n"
     ]
    }
   ],
   "source": [
    "# The algorithms we want to ensemble.\n",
    "# We're using the more linear predictors for the logistic regression, and everything with the gradient boosting classifier.\n",
    "# predictors = [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\"]\n",
    "\n",
    "algorithms = [\n",
    "    [GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), predictors],\n",
    "    [LogisticRegression(random_state=1), [\"Pclass\", \"Sex\", \"Fare\", \"FamilySize\", \"Title\", \"Age\", \"Embarked\"]]\n",
    "]\n",
    "\n",
    "# Initialize the cross validation folds\n",
    "kf = KFold(df.shape[0], n_folds=3, random_state=1)\n",
    "\n",
    "predictions = []\n",
    "for train, test in kf:\n",
    "    train_target = df[\"Survived\"].iloc[train]\n",
    "    full_test_predictions = []\n",
    "    # Make predictions for each algorithm on each fold\n",
    "    for alg, predictors in algorithms:\n",
    "        # Fit the algorithm on the training data.\n",
    "        alg.fit(df[predictors].iloc[train,:], train_target)\n",
    "        # Select and predict on the test fold.  \n",
    "        # The .astype(float) is necessary to convert the dataframe to all floats and avoid an sklearn error.\n",
    "        test_predictions = alg.predict_proba(df[predictors].iloc[test,:].astype(float))[:,1]\n",
    "        full_test_predictions.append(test_predictions)\n",
    "    # Use a simple ensembling scheme -- just average the predictions to get the final classification.\n",
    "    test_predictions = (full_test_predictions[0] + full_test_predictions[1]) / 2\n",
    "    # Any value over .5 is assumed to be a 1 prediction, and below .5 is a 0 prediction.\n",
    "    test_predictions[test_predictions <= .5] = 0\n",
    "    test_predictions[test_predictions > .5] = 1\n",
    "    predictions.append(test_predictions)\n",
    "\n",
    "# Put all the predictions together into one array.\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# Compute accuracy by comparing to the training data.\n",
    "accuracy = sum(predictions[predictions == df[\"Survived\"]]) / len(predictions)\n",
    "print accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the kaggle submission csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testf = pd.read_csv('test.csv', header=0)\n",
    "testf = preprocess(testf)\n",
    "\n",
    "full_predictions = []\n",
    "for alg, predictors in algorithms:\n",
    "    # Fit the algorithm using the full training data.\n",
    "    alg.fit(df[predictors], df[\"Survived\"])\n",
    "    # Predict using the test dataset.  We have to convert all the columns to floats to avoid an error.\n",
    "    predictions = alg.predict_proba(testf[predictors].astype(float))[:,1]\n",
    "    full_predictions.append(predictions)\n",
    "\n",
    "# The gradient boosting classifier generates better predictions, so we weight it higher.\n",
    "predictions = (full_predictions[0] * 3 + full_predictions[1]) / 4\n",
    "\n",
    "predictions[predictions <= .5] = 0\n",
    "predictions[predictions > .5] = 1\n",
    "predictions = predictions.astype(int)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"PassengerId\": testf[\"PassengerId\"],\n",
    "    \"Survived\": predictions\n",
    "})\n",
    "\n",
    "submission.to_csv(\"kaggle.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
