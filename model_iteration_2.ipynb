{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating deaths on the titanic\n",
    "This notebook cleans the data from the titanic kaggle competition and then uses 3 alorithms to try and predict who survived (Logistic Regression, Random Forest, Gradient Boosting). Much of the code was copied from or inspired by these 2 sources: https://www.dataquest.io/mission/75/improving-your-submission and https://github.com/elenacuoco/kaggle-competitions/blob/master/Titanic-For_Blog.ipynb. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import operator\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "enc=preprocessing.OneHotEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rawdf=pd.read_csv(\"train.csv\")\n",
    "rawdf_test=pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cleaning functions (source: https://github.com/elenacuoco/kaggle-competitions/blob/master/Titanic-For_Blog.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###utility to clean and munge data\n",
    "def substrings_in_string(big_string, substrings):\n",
    "    for substring in substrings:\n",
    "        if string.find(big_string, substring) != -1:\n",
    "            return substring\n",
    "    print big_string\n",
    "    return np.nan\n",
    "\n",
    "# A dictionary mapping family name to id\n",
    "family_id_mapping = {}\n",
    "\n",
    "def clean_and_munge_data(df):\n",
    "    #setting silly values to nan\n",
    "    df.Fare = df.Fare.map(lambda x: np.nan if x==0 else x)\n",
    "    \n",
    "    #Creating new family_size column\n",
    "    df['Family_Size']=df['SibSp']+df['Parch']\n",
    "    df['Family']=df['SibSp']*df['Parch']\n",
    "    \n",
    "    #creating a title column from name\n",
    "    title_list=['Mrs', 'Mr', 'Master', 'Miss', 'Major', 'Rev',\n",
    "                'Dr', 'Ms', 'Mlle','Col', 'Capt', 'Mme', 'Countess',\n",
    "                'Don', 'Jonkheer']\n",
    "    df['Title']=df['Name'].map(lambda x: substrings_in_string(x, title_list))\n",
    "\n",
    "    #replacing all titles with mr, mrs, miss, master\n",
    "    def replace_titles(x):\n",
    "        title=x['Title']\n",
    "        if title in ['Mr','Don', 'Major', 'Capt', 'Jonkheer', 'Rev', 'Col']:\n",
    "            return 'Mr'\n",
    "        elif title in ['Master']:\n",
    "            return 'Master'\n",
    "        elif title in ['Countess', 'Mme','Mrs']:\n",
    "            return 'Mrs'\n",
    "        elif title in ['Mlle', 'Ms','Miss']:\n",
    "            return 'Miss'\n",
    "        elif title =='Dr':\n",
    "            if x['Sex']=='Male':\n",
    "                return 'Mr'\n",
    "            else:\n",
    "                return 'Mrs'\n",
    "        elif title =='':\n",
    "            if x['Sex']=='Male':\n",
    "                return 'Master'\n",
    "            else:\n",
    "                return 'Miss'\n",
    "        else:\n",
    "            return title\n",
    "        \n",
    "    ##Family Grouping code taken from: https://www.dataquest.io/mission/75/improving-your-submission    \n",
    "\n",
    "    # A function to get the id given a row\n",
    "    def get_family_id(row):\n",
    "        # Find the last name by splitting on a comma\n",
    "        last_name = row[\"Name\"].split(\",\")[0]\n",
    "        # Create the family id\n",
    "        family_id = \"{0}{1}\".format(last_name, row[\"Family_Size\"])\n",
    "        # Look up the id in the mapping\n",
    "        if family_id not in family_id_mapping:\n",
    "            if len(family_id_mapping) == 0:\n",
    "                current_id = 1\n",
    "            else:\n",
    "                # Get the maximum id from the mapping and add one to it if we don't have an id\n",
    "                current_id = (max(family_id_mapping.items(), key=operator.itemgetter(1))[1] + 1)\n",
    "            family_id_mapping[family_id] = current_id\n",
    "        return family_id_mapping[family_id]\n",
    "    \n",
    "    # Get the family ids with the apply method\n",
    "    family_ids = df.apply(get_family_id, axis=1)\n",
    "\n",
    "    # There are a lot of family ids, so we'll compress all of the families under 3 members into one code.\n",
    "    family_ids[df[\"Family_Size\"] < 3] = -1\n",
    "\n",
    "    # Print the count of each unique id.\n",
    "    print(pd.value_counts(family_ids))\n",
    "\n",
    "    df[\"FamilyId\"] = family_ids\n",
    "\n",
    "    df['Title']=df.apply(replace_titles, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #imputing nan values\n",
    "    df.loc[ (df.Fare.isnull())&(df.Pclass==1),'Fare'] =np.median(df[df['Pclass'] == 1]['Fare'].dropna())\n",
    "    df.loc[ (df.Fare.isnull())&(df.Pclass==2),'Fare'] =np.median( df[df['Pclass'] == 2]['Fare'].dropna())\n",
    "    df.loc[ (df.Fare.isnull())&(df.Pclass==3),'Fare'] = np.median(df[df['Pclass'] == 3]['Fare'].dropna())\n",
    "\n",
    "    df['AgeFill']=df['Age']\n",
    "    mean_ages = np.zeros(4)\n",
    "    mean_ages[0]=np.average(df[df['Title'] == 'Miss']['Age'].dropna())\n",
    "    mean_ages[1]=np.average(df[df['Title'] == 'Mrs']['Age'].dropna())\n",
    "    mean_ages[2]=np.average(df[df['Title'] == 'Mr']['Age'].dropna())\n",
    "    mean_ages[3]=np.average(df[df['Title'] == 'Master']['Age'].dropna())\n",
    "    df.loc[ (df.Age.isnull()) & (df.Title == 'Miss') ,'AgeFill'] = mean_ages[0]\n",
    "    df.loc[ (df.Age.isnull()) & (df.Title == 'Mrs') ,'AgeFill'] = mean_ages[1]\n",
    "    df.loc[ (df.Age.isnull()) & (df.Title == 'Mr') ,'AgeFill'] = mean_ages[2]\n",
    "    df.loc[ (df.Age.isnull()) & (df.Title == 'Master') ,'AgeFill'] = mean_ages[3]\n",
    "\n",
    "    df['AgeCat']=df['AgeFill']\n",
    "    df.loc[ (df.AgeFill<=10) ,'AgeCat'] = 'child'\n",
    "    df.loc[ (df.AgeFill>60),'AgeCat'] = 'aged'\n",
    "    df.loc[ (df.AgeFill>10) & (df.AgeFill <=30) ,'AgeCat'] = 'adult'\n",
    "    df.loc[ (df.AgeFill>30) & (df.AgeFill <=60) ,'AgeCat'] = 'senior'\n",
    "\n",
    "    df.Embarked = df.Embarked.fillna('S')\n",
    "\n",
    "\n",
    "    #Special case for cabins as nan may be signal\n",
    "    df.loc[ df.Cabin.isnull()==True,'Cabin'] = 0.5\n",
    "    df.loc[ df.Cabin.isnull()==False,'Cabin'] = 1.5\n",
    "   #Fare per person\n",
    "\n",
    "    df['Fare_Per_Person']=df['Fare']/(df['Family_Size']+1)\n",
    "\n",
    "    #Age times class\n",
    "\n",
    "    df['AgeClass']=df['AgeFill']*df['Pclass']\n",
    "    df['ClassFare']=df['Pclass']*df['Fare_Per_Person']\n",
    "\n",
    "\n",
    "    df['HighLow']=df['Pclass']\n",
    "    df.loc[ (df.Fare_Per_Person<8) ,'HighLow'] = 'Low'\n",
    "    df.loc[ (df.Fare_Per_Person>=8) ,'HighLow'] = 'High'\n",
    "    \n",
    "    #df['Title']=df['Name'].map(lambda x: substrings_in_string(x, title_list))\n",
    "\n",
    "    le.fit(df['Sex'] )\n",
    "    x_sex=le.transform(df['Sex'])\n",
    "    df['Sex']=x_sex.astype(np.float)\n",
    "\n",
    "    le.fit( df['Ticket'])\n",
    "    x_Ticket=le.transform( df['Ticket'])\n",
    "    df['Ticket']=x_Ticket.astype(np.float)\n",
    "\n",
    "    le.fit(df['Title'])\n",
    "    x_title=le.transform(df['Title'])\n",
    "    df['Title'] =x_title.astype(np.float)\n",
    "\n",
    "    le.fit(df['HighLow'])\n",
    "    x_hl=le.transform(df['HighLow'])\n",
    "    df['HighLow']=x_hl.astype(np.float)\n",
    "\n",
    "\n",
    "    le.fit(df['AgeCat'])\n",
    "    x_age=le.transform(df['AgeCat'])\n",
    "    df['AgeCat'] =x_age.astype(np.float)\n",
    "\n",
    "    le.fit(df['Embarked'])\n",
    "    x_emb=le.transform(df['Embarked'])\n",
    "    df['Embarked']=x_emb.astype(np.float)\n",
    "\n",
    "    df = df.drop(['Name','Age','Cabin'], axis=1) #remove Name,Age and PassengerId\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the cleaning function to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1      800\n",
      " 14       8\n",
      " 149      7\n",
      " 63       6\n",
      " 50       6\n",
      " 59       6\n",
      " 17       5\n",
      " 384      4\n",
      " 27       4\n",
      " 25       4\n",
      " 162      4\n",
      " 8        4\n",
      " 84       4\n",
      " 340      4\n",
      " 43       3\n",
      " 269      3\n",
      " 58       3\n",
      " 633      2\n",
      " 167      2\n",
      " 280      2\n",
      " 510      2\n",
      " 90       2\n",
      " 83       1\n",
      " 625      1\n",
      " 376      1\n",
      " 449      1\n",
      " 498      1\n",
      " 588      1\n",
      "dtype: int64\n",
      "-1      384\n",
      " 149      4\n",
      " 25       3\n",
      " 280      3\n",
      " 27       2\n",
      " 59       2\n",
      " 633      2\n",
      " 510      2\n",
      " 167      2\n",
      " 90       2\n",
      " 162      1\n",
      " 759      1\n",
      " 449      1\n",
      " 84       1\n",
      " 269      1\n",
      " 58       1\n",
      " 43       1\n",
      " 794      1\n",
      " 918      1\n",
      " 17       1\n",
      " 14       1\n",
      " 8        1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df=clean_and_munge_data(rawdf)\n",
    "df_test=clean_and_munge_data(rawdf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Family_Size</th>\n",
       "      <th>Family</th>\n",
       "      <th>Title</th>\n",
       "      <th>FamilyId</th>\n",
       "      <th>AgeFill</th>\n",
       "      <th>AgeCat</th>\n",
       "      <th>Fare_Per_Person</th>\n",
       "      <th>AgeClass</th>\n",
       "      <th>ClassFare</th>\n",
       "      <th>HighLow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>0.647587</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>338.528620</td>\n",
       "      <td>32.689318</td>\n",
       "      <td>1.536476</td>\n",
       "      <td>0.904602</td>\n",
       "      <td>0.567901</td>\n",
       "      <td>1.860831</td>\n",
       "      <td>14.232323</td>\n",
       "      <td>29.819131</td>\n",
       "      <td>1.591470</td>\n",
       "      <td>20.401486</td>\n",
       "      <td>65.062477</td>\n",
       "      <td>32.118852</td>\n",
       "      <td>0.417508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>257.353842</td>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>0.477990</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>200.850657</td>\n",
       "      <td>49.611639</td>\n",
       "      <td>0.791503</td>\n",
       "      <td>1.613459</td>\n",
       "      <td>1.979287</td>\n",
       "      <td>0.721066</td>\n",
       "      <td>69.886368</td>\n",
       "      <td>13.285423</td>\n",
       "      <td>1.428952</td>\n",
       "      <td>35.894413</td>\n",
       "      <td>33.676295</td>\n",
       "      <td>35.845210</td>\n",
       "      <td>0.493425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.012500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.132143</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>3.396429</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>223.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>158.500000</td>\n",
       "      <td>7.925000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>21.835616</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.589600</td>\n",
       "      <td>39.500000</td>\n",
       "      <td>21.545883</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>337.000000</td>\n",
       "      <td>14.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>8.662500</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>24.150000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>668.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>519.500000</td>\n",
       "      <td>31.275000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>35.841667</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>24.500000</td>\n",
       "      <td>91.750000</td>\n",
       "      <td>28.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>680.000000</td>\n",
       "      <td>512.329200</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>633.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>512.329200</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>512.329200</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId    Survived      Pclass         Sex       SibSp  \\\n",
       "count   891.000000  891.000000  891.000000  891.000000  891.000000   \n",
       "mean    446.000000    0.383838    2.308642    0.647587    0.523008   \n",
       "std     257.353842    0.486592    0.836071    0.477990    1.102743   \n",
       "min       1.000000    0.000000    1.000000    0.000000    0.000000   \n",
       "25%     223.500000    0.000000    2.000000    0.000000    0.000000   \n",
       "50%     446.000000    0.000000    3.000000    1.000000    0.000000   \n",
       "75%     668.500000    1.000000    3.000000    1.000000    1.000000   \n",
       "max     891.000000    1.000000    3.000000    1.000000    8.000000   \n",
       "\n",
       "            Parch      Ticket        Fare    Embarked  Family_Size  \\\n",
       "count  891.000000  891.000000  891.000000  891.000000   891.000000   \n",
       "mean     0.381594  338.528620   32.689318    1.536476     0.904602   \n",
       "std      0.806057  200.850657   49.611639    0.791503     1.613459   \n",
       "min      0.000000    0.000000    4.012500    0.000000     0.000000   \n",
       "25%      0.000000  158.500000    7.925000    1.000000     0.000000   \n",
       "50%      0.000000  337.000000   14.500000    2.000000     0.000000   \n",
       "75%      0.000000  519.500000   31.275000    2.000000     1.000000   \n",
       "max      6.000000  680.000000  512.329200    2.000000    10.000000   \n",
       "\n",
       "           Family       Title    FamilyId     AgeFill      AgeCat  \\\n",
       "count  891.000000  891.000000  891.000000  891.000000  891.000000   \n",
       "mean     0.567901    1.860831   14.232323   29.819131    1.591470   \n",
       "std      1.979287    0.721066   69.886368   13.285423    1.428952   \n",
       "min      0.000000    0.000000   -1.000000    0.420000    0.000000   \n",
       "25%      0.000000    2.000000   -1.000000   21.835616    0.000000   \n",
       "50%      0.000000    2.000000   -1.000000   30.000000    2.000000   \n",
       "75%      0.000000    2.000000   -1.000000   35.841667    3.000000   \n",
       "max     16.000000    3.000000  633.000000   80.000000    3.000000   \n",
       "\n",
       "       Fare_Per_Person    AgeClass   ClassFare     HighLow  \n",
       "count       891.000000  891.000000  891.000000  891.000000  \n",
       "mean         20.401486   65.062477   32.118852    0.417508  \n",
       "std          35.894413   33.676295   35.845210    0.493425  \n",
       "min           1.132143    0.920000    3.396429    0.000000  \n",
       "25%           7.589600   39.500000   21.545883    0.000000  \n",
       "50%           8.662500   63.000000   24.150000    0.000000  \n",
       "75%          24.500000   91.750000   28.500000    1.000000  \n",
       "max         512.329200  222.000000  512.329200    1.000000  "
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression Model. Kaggle Score: (Don't have enough submissions to investigate, probably around 0.76077)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.792404097151\n",
      "148\n"
     ]
    }
   ],
   "source": [
    "# The columns we'll use to predict the target\n",
    "predictorsLog = [\"Pclass\", \"Sex\", \"AgeCat\", \"AgeFill\", \"Family_Size\",\"Fare_Per_Person\", \"Fare\", \"Embarked\"]\n",
    "\n",
    "# Initialize our algorithm\n",
    "alg = LogisticRegression(random_state=1)\n",
    "# Compute the accuracy score for all the cross validation folds.  (much simpler than what we did before!)\n",
    "scores = cross_validation.cross_val_score(alg, df[predictorsLog], df[\"Survived\"], cv=10)\n",
    "# Take the mean of the scores (because we have one for each fold)\n",
    "print scores.mean()\n",
    "\n",
    "# Train the algorithm using all the training data\n",
    "alg.fit(df[predictorsLog], df[\"Survived\"])\n",
    "\n",
    "# Make predictions using the test set.\n",
    "predictionsLog = alg.predict_proba(df_test[predictorsLog])[:,1]\n",
    "\n",
    "# Create a new dataframe with only the columns Kaggle wants from the dataset.\n",
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": df_test[\"PassengerId\"],\n",
    "        \"Survived\": np.round(predictionsLog).astype(int)\n",
    "    })\n",
    "print sum(submission.Survived)\n",
    "submission.to_csv(\"kaggleLogReg.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an attempt to use a random forest classifier. Results on Kaggle: 0.79904. Without the algorithm's parameters the results were only 0.72727 on Kaggle. Using 5000 estimators and max_depth 10 the kaggle score reduced to 0.77990."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.837261945296\n",
      "148\n"
     ]
    }
   ],
   "source": [
    "# The columns we'll use to predict the target\n",
    "predictorsRan = [\"Pclass\", \"Sex\", \"AgeCat\", \"AgeFill\", \"Family_Size\", \"Fare_Per_Person\", \"Fare\", \"Embarked\", \"Title\", \"ClassFare\", \"FamilyId\"]\n",
    "\n",
    "# Initialize our algorithm (The parameters are taken from: https://github.com/elenacuoco/kaggle-competitions/blob/master/Titanic-For_Blog.ipynb)\n",
    "alg = RandomForestClassifier(n_estimators=350, criterion='entropy', max_depth=5, min_samples_split=2,\n",
    "  min_samples_leaf=2, max_features='auto',    bootstrap=False, oob_score=False, n_jobs=1, random_state=2,\n",
    "  verbose=0)\n",
    "# Compute the accuracy score for all the cross validation folds.  (much simpler than what we did before!)\n",
    "scores = cross_validation.cross_val_score(alg, df[predictorsRan], df[\"Survived\"], cv=10)\n",
    "# Take the mean of the scores (because we have one for each fold)\n",
    "print scores.mean()\n",
    "# Train the algorithm using all the training data\n",
    "alg.fit(df[predictorsRan], df[\"Survived\"])\n",
    "\n",
    "# Make predictions using the test set.\n",
    "predictionsRan = alg.predict_proba(df_test[predictorsRan])[:,1]\n",
    "\n",
    "# Create a new dataframe with only the columns Kaggle wants from the dataset.\n",
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": df_test[\"PassengerId\"],\n",
    "        \"Survived\": np.round(predictionsRan).astype(int)\n",
    "    })\n",
    "print sum(submission.Survived)\n",
    "submission.to_csv(\"kaggleRanFor.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boost Kaggle Score: 0.78947"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.827212007718\n",
      "143\n"
     ]
    }
   ],
   "source": [
    "# The columns we'll use to predict the target\n",
    "predictorsGrad = [\"Pclass\", \"Sex\", \"AgeFill\", \"Family_Size\", \"Fare\", \"Embarked\", \"Title\", \"FamilyId\"]\n",
    "\n",
    "# Initialize our algorithm (The parameters are taken from: https://github.com/elenacuoco/kaggle-competitions/blob/master/Titanic-For_Blog.ipynb)\n",
    "alg = GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3)\n",
    "# Compute the accuracy score for all the cross validation folds.  (much simpler than what we did before!)\n",
    "scores = cross_validation.cross_val_score(alg, df[predictorsGrad], df[\"Survived\"], cv=10)\n",
    "# Take the mean of the scores (because we have one for each fold)\n",
    "print scores.mean()\n",
    "# Train the algorithm using all the training data\n",
    "alg.fit(df[predictorsGrad], df[\"Survived\"])\n",
    "\n",
    "# Make predictions using the test set.\n",
    "predictionsGrad = alg.predict_proba(df_test[predictorsGrad])[:,1]\n",
    "\n",
    "# Create a new dataframe with only the columns Kaggle wants from the dataset.\n",
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": df_test[\"PassengerId\"],\n",
    "        \"Survived\": np.round(predictionsGrad).astype(int)\n",
    "    })\n",
    "print sum(submission.Survived)\n",
    "submission.to_csv(\"kaggleGrad.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Ensemble Prediction (Logistic regression, Random Forest, GradientBoosting). Overall the Random forest is the best model and so combining it with the others brought down the score to 0.79426 compared to 0.79904 with the Random Forest alone. While I would have hoped that the ensemble would beat the Random Forest alone, it makes sense that the others would bring down the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149\n"
     ]
    }
   ],
   "source": [
    "# Create a new dataframe with only the columns Kaggle wants from the dataset.\n",
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": df_test[\"PassengerId\"],\n",
    "        \"Survived\": np.round((3*predictionsRan + predictionsLog + predictionsGrad)/5).astype(int)\n",
    "    })\n",
    "submission.to_csv(\"kaggleEnsem.csv\", index=False)\n",
    "print sum(submission.Survived)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the future I would like to try the ensemble method but having each algorithm vote instead of averaging the results. I think it would also be cool if I could analyse the ethnicity of the names and see if that correlated with survival. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
