{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I read through both Dataquest tutorials, as well as some more tutorials on the kaggle forums. All of them seemed to revolve around using the same variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using age, like most of the tutorials, I've turned that continuous variable into a discrete 'Adult' variable, based on whether a passenger was 16 or older. I use a FamilySize variable that is the sum of SibSp and Parch, and I've turned the Embarked and Sex categories into integers instead of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas, math\n",
    "import numpy as np\n",
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "titanic = pandas.read_csv('train.csv')\n",
    "titanic_test = pandas.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up data, replace strings with integers, create new variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titanic['FamSize'] = titanic.SibSp + titanic.Parch\n",
    "titanic_test['FamSize'] = titanic_test.SibSp + titanic_test.Parch\n",
    "\n",
    "titanic.Sex.replace(['male', 'female'], [0, 1], inplace=True)\n",
    "titanic_test.Sex.replace(['male', 'female'], [0, 1], inplace=True)\n",
    "\n",
    "titanic['Age'] = titanic['Age'].fillna(titanic['Age'].median())\n",
    "titanic_test['Age'] = titanic_test['Age'].fillna(titanic['Age'].median())\n",
    "\n",
    "titanic['LogAge'] = titanic['Age'].apply(lambda x: math.log(x))\n",
    "titanic_test['LogAge'] = titanic_test['Age'].apply(lambda x: math.log(x))\n",
    "\n",
    "titanic['NameLen'] = titanic['Name'].apply(lambda x: len(x))\n",
    "titanic_test['NameLen'] = titanic['Name'].apply(lambda x: len(x))\n",
    "\n",
    "titanic_test['Fare'].fillna(titanic_test['Fare'].median(), inplace=True)\n",
    "\n",
    "titanic['Adult'] = titanic['Age'].apply(lambda(x): 0 if x<16.0 else 1)\n",
    "titanic_test['Adult'] = titanic_test['Age'].apply(lambda(x): 0 if x<16.0 else 1)\n",
    "\n",
    "titanic.Embarked.fillna('S', inplace=True)\n",
    "titanic_test.Embarked.fillna('S', inplace=True)\n",
    "\n",
    "titanic['Embarked'].replace(['S', 'C', 'Q'], [0, 1, 2], inplace=True)\n",
    "titanic_test['Embarked'].replace(['S', 'C', 'Q'], [0, 1, 2], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a logistic regression algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.79012345679\n"
     ]
    }
   ],
   "source": [
    "predictors = [\"Sex\", \"Adult\", \"Embarked\", \"FamSize\"]\n",
    "alg = LogisticRegression(random_state=1)\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=3)\n",
    "print scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a Random Forest algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.824915824916\n"
     ]
    }
   ],
   "source": [
    "alg = RandomForestClassifier(random_state=1, n_estimators=120, min_samples_split=3, min_samples_leaf=4)\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=3)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Besides using different variables as parameters in the Random Forest, I also tried to modify the parameters of the Random Forest, to little effect. The most dramatic effect was in modifying the number of estimators, and it almost always worsened the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alg.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "predictions = alg.predict(titanic_test[predictors])\n",
    "\n",
    "submission = pandas.DataFrame({\n",
    "        \"PassengerId\": titanic_test[\"PassengerId\"],\n",
    "        \"Survived\": predictions\n",
    "    })\n",
    "    \n",
    "submission.to_csv(\"kaggle.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best score = .78469 using a Random Forest with Sex, Adult, Embarked, FamSize, 120 estimators, minimum split of 3, minimum sample of 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.79797979798\n"
     ]
    }
   ],
   "source": [
    "algorithms = [\n",
    "    [GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), predictors],\n",
    "    [LogisticRegression(random_state=1), predictors]\n",
    "]\n",
    "\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=3)\n",
    "print(scores.mean())\n",
    "\n",
    "full_predictions = []\n",
    "for alg, predictors in algorithms:\n",
    "    alg.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "    predictions = alg.predict_proba(titanic_test[predictors].astype(float))[:,1]\n",
    "    full_predictions.append(predictions)\n",
    "\n",
    "# The gradient boosting classifier generates better predictions, so we weight it higher.\n",
    "predictions = (full_predictions[0] * 3 + full_predictions[1]) / 4\n",
    "predictions[predictions>.5] = 1\n",
    "predictions[predictions<=.5] =  0\n",
    "predictions = predictions.astype(int)\n",
    "submission = pandas.DataFrame({'PassengerId': titanic_test['PassengerId'], 'Survived':predictions})\n",
    "submission.to_csv(\"kaggle.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best score = .78947 using an ensemble of Gradient Boosting and Logistic Regression. Parameters were Sex, Adult, Embarked, FamSize, and Pclass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was very difficult to correlate an improvement in training data score with a correlation in kaggle submission scores. When I was using the Random Forest, reducing the number of parameters almost always gave a better performance (up to a point). This makes sense because I think complex models tend to overfit to the training data. I also just read The Evolution of Cooperation by Robert Axelrod, and that book gives plenty of examples for why models should be as simple as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
