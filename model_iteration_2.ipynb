{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beginning thoughts\n",
    "\n",
    "I looked through several options for model_iteration_2: DataQuest Mission 75, the blog posts of several other kaggle competition participants, and scanning the forums for ideas. Afer completing DataQuest Mission 75 in depth I think that it provides the most comprehensive starting point for model_iteration_2, and I really appreciate the opportunity to both read code and implement ideas (a rare opportunity since the titanic dataset in particular is an educational dataset?). Hopefully the work done in this notebook becomes a resource of examples for projects in the near future; the lambda functions passed into .apply and the regular expression usage in particular are not things that I use commonly in python. \n",
    "\n",
    "In this notebook I will explain and use the code that was suggested in DataQuest Mission 75, and adapt it to implement additional ideas. The markdown cells also include some reflection on the process and how I reacted to the dataquest mission. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall Themes of DataQuest Mission 75:\n",
    "\n",
    "* Use a better machine learning algorithm.\n",
    "* Generate better features.\n",
    "* Combine multiple machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titanic = pandas.read_csv(\"./data/train.csv\")\n",
    "titanic_test = pandas.read_csv(\"./data/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean up the training data (same as what we did in model_iteration_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Replace the missing age values with the median age\n",
    "titanic[\"Age\"] = titanic[\"Age\"].fillna(titanic[\"Age\"].median())\n",
    "\n",
    "# From genders to numbers\n",
    "titanic.loc[titanic[\"Sex\"] == \"male\", \"Sex\"] = 0\n",
    "titanic.loc[titanic[\"Sex\"] == \"female\", \"Sex\"] = 1\n",
    "\n",
    "# From embarked letters to numbers\n",
    "titanic[\"Embarked\"] = titanic[\"Embarked\"].fillna(\"S\")\n",
    "titanic.loc[titanic[\"Embarked\"] == \"S\", \"Embarked\"] = 0\n",
    "titanic.loc[titanic[\"Embarked\"] == \"C\", \"Embarked\"] = 1\n",
    "titanic.loc[titanic[\"Embarked\"] == \"Q\", \"Embarked\"] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean up the test data (same as what we did in model_iteration_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic_test = pandas.read_csv(\"./data/test.csv\")\n",
    "\n",
    "# Age column\n",
    "titanic_test[\"Age\"] = titanic_test[\"Age\"].fillna(titanic[\"Age\"].median())\n",
    "\n",
    "# Sex column\n",
    "titanic_test.loc[titanic_test[\"Sex\"] == \"male\", \"Sex\"] = 0\n",
    "titanic_test.loc[titanic_test[\"Sex\"] == \"female\", \"Sex\"] = 1\n",
    "\n",
    "# Embarked column\n",
    "titanic_test[\"Embarked\"] = titanic_test[\"Embarked\"].fillna(\"S\")\n",
    "titanic_test.loc[titanic_test[\"Embarked\"] == \"S\", \"Embarked\"] = 0\n",
    "titanic_test.loc[titanic_test[\"Embarked\"] == \"C\", \"Embarked\"] = 1\n",
    "titanic_test.loc[titanic_test[\"Embarked\"] == \"Q\", \"Embarked\"] = 2\n",
    "\n",
    "# Fare column\n",
    "titanic_test[\"Fare\"] = titanic_test[\"Fare\"].fillna(titanic[\"Fare\"].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forests!\n",
    "\n",
    "Random forests have the ability to capture many different \"layers\" of relationships between the features in our dataset. I use the word \"layers\" here to loosely describe the different branches of decision trees in the random forest. Random forests are random because each decision tree in the forest gets a random subset of the data. Taking the average of the results from the trees will then result in the model's prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.801346801347\n"
     ]
    }
   ],
   "source": [
    "# Code from DataQuest mission 75\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "\n",
    "# Initialize our algorithm with the default parameters\n",
    "# n_estimators is the number of trees we want to make\n",
    "# min_samples_split is the minimum number of rows we need to make a split\n",
    "# min_samples_leaf is the minimum number of samples we can have at the place where a tree branch ends (the bottom points of the tree)\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=10, min_samples_split=2, min_samples_leaf=1)\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"])\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first stab with random forests resulted in 80% accuracy, which is better than the linear and logistic regression in model_iteration_1. Based on what I know about these three algorithms, I can see how random forests do a better job of capturing the complicated relationships between features in this dataset -- for example, the branches can capture how the same non-sex features can impact men and women differently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of people who use random forest models in their implementation are also a fan of visualizing a tree from their random forest, such as the picture included in [triangleinequality's tutorial](https://triangleinequality.wordpress.com/2013/09/05/a-complete-guide-to-getting-0-79903-in-kaggles-titanic-competition-with-python/) and shown immediately below. \n",
    "\n",
    "![a tree in the random forest](https://triangleinequality.files.wordpress.com/2013/09/sample_tree1.png?w=960&h=960)\n",
    "\n",
    "The blue nodes are the leaves that represent whether that path of the tree resulted in survival or death. The dataquest mission also explained some of the relevant \"high-level\" parameters of the random forest model, such as the amount of splits and how many samples are needed to create a leaf (try tweaking `min_samples_split` and `min_samples_leaf`). There is probably a fair amount of iteration involved in finding the sweet spot of branches/layers of the decision tree (too many branches will result in overfitting on the training dataset, and too little branches will result in a ineffective set of decision trees). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making new features\n",
    "\n",
    "Bulletpoints of suggestions from the dataquest page:\n",
    "* The length of the name -- this could pertain to how rich the \n",
    "person was, and therefore their position in the Titanic.\n",
    "* The total number of people in a family (SibSp + Parch).\n",
    "\n",
    "Emily's commentary:   \n",
    "* I'm not sure if the first bulletpoint would make a nontrivial improvement, since the wealth of the passengers could already be represented with the `Pclass` variable, and names have a ton of variation on their own. We can confirm this with a correlation coefficient or some other measure for relevance of a feature on the prediction.    \n",
    "* I think the second bulletpoint would result in an improvement to the model. One potential story behind the family feature is that people with families stick together and help each other escape. Another potential story is that large families may have a tough time trying to get everyone safe because of all the craziness of the event and the many people they are trying to take care of at once. It also makes more sense to put family as one feature rather than `SibSp` and `Parch` separately if they both help the model \"figure out the family situation\" -- maybe the curse of dimensionality is showing up here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code from DataQuest mission 75\n",
    "\n",
    "# Generating a familysize column\n",
    "titanic[\"FamilySize\"] = titanic[\"SibSp\"] + titanic[\"Parch\"]\n",
    "\n",
    "# The .apply method generates a new series\n",
    "titanic[\"NameLength\"] = titanic[\"Name\"].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I usually don't use lambda functions in python, so I super appreciated the quick example above with the `.apply` method. I'll be sure to remember that when I'm working with dataframes in the future!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataquest mission used regular expressions to extract the titles from the names of the passengers -- this idea was also mentioned in the class discussion in Data Science last week. Similarly to the lambda used above, I also don't have a ton of experience with regular expressions, and appreciate this example! Definitely makes this notebook a valuable resource for future projects if I still need the examples in the near future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr          517\n",
      "Miss        182\n",
      "Mrs         125\n",
      "Master       40\n",
      "Dr            7\n",
      "Rev           6\n",
      "Col           2\n",
      "Major         2\n",
      "Mlle          2\n",
      "Countess      1\n",
      "Ms            1\n",
      "Lady          1\n",
      "Jonkheer      1\n",
      "Don           1\n",
      "Mme           1\n",
      "Capt          1\n",
      "Sir           1\n",
      "Name: Name, dtype: int64\n",
      "1     517\n",
      "2     183\n",
      "3     125\n",
      "4      40\n",
      "5       7\n",
      "6       6\n",
      "7       5\n",
      "10      3\n",
      "8       3\n",
      "9       2\n",
      "Name: Name, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Code from DataQuest mission 75\n",
    "\n",
    "import re\n",
    "\n",
    "# A function to get the title from a name.\n",
    "def get_title(name):\n",
    "    # Use a regular expression to search for a title.  Titles always consist of capital and lowercase letters, and end with a period.\n",
    "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "    # If the title exists, extract and return it.\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return \"\"\n",
    "\n",
    "# Get all the titles and print how often each one occurs.\n",
    "titles = titanic[\"Name\"].apply(get_title)\n",
    "print(pandas.value_counts(titles))\n",
    "\n",
    "# Map each title to an integer.  Some titles are very rare, and are compressed into the same codes as other titles.\n",
    "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Dr\": 5, \"Rev\": 6, \"Major\": 7, \"Col\": 7, \"Mlle\": 8, \"Mme\": 8, \"Don\": 9, \"Lady\": 10, \"Countess\": 10, \"Jonkheer\": 10, \"Sir\": 9, \"Capt\": 7, \"Ms\": 2}\n",
    "for k,v in title_mapping.items():\n",
    "    titles[titles == k] = v\n",
    "\n",
    "# Verify that we converted everything.\n",
    "print(pandas.value_counts(titles))\n",
    "\n",
    "# Add in the title column.\n",
    "titanic[\"Title\"] = titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataquest mission dives deeper into the \"family\" storyline by making family groups. While this part of the dataquest mission didn't prompt me to write any additional code, I super appreciated the example of a working implementation to read through and understand at my own pace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1      800\n",
      " 14       8\n",
      " 149      7\n",
      " 63       6\n",
      " 50       6\n",
      " 59       6\n",
      " 17       5\n",
      " 384      4\n",
      " 27       4\n",
      " 25       4\n",
      " 162      4\n",
      " 8        4\n",
      " 84       4\n",
      " 340      4\n",
      " 43       3\n",
      " 269      3\n",
      " 58       3\n",
      " 633      2\n",
      " 167      2\n",
      " 280      2\n",
      " 510      2\n",
      " 90       2\n",
      " 83       1\n",
      " 625      1\n",
      " 376      1\n",
      " 449      1\n",
      " 498      1\n",
      " 588      1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Code from DataQuest mission 75\n",
    "\n",
    "import operator\n",
    "\n",
    "# A dictionary mapping family name to id\n",
    "family_id_mapping = {}\n",
    "\n",
    "# A function to get the id given a row\n",
    "def get_family_id(row):\n",
    "    # Find the last name by splitting on a comma\n",
    "    last_name = row[\"Name\"].split(\",\")[0]\n",
    "    # Create the family id\n",
    "    family_id = \"{0}{1}\".format(last_name, row[\"FamilySize\"])\n",
    "    # Look up the id in the mapping\n",
    "    if family_id not in family_id_mapping:\n",
    "        if len(family_id_mapping) == 0:\n",
    "            current_id = 1\n",
    "        else:\n",
    "            # Get the maximum id from the mapping and add one to it if we don't have an id\n",
    "            current_id = (max(family_id_mapping.items(), key=operator.itemgetter(1))[1] + 1)\n",
    "        family_id_mapping[family_id] = current_id\n",
    "    return family_id_mapping[family_id]\n",
    "\n",
    "# Get the family ids with the apply method\n",
    "family_ids = titanic.apply(get_family_id, axis=1)\n",
    "\n",
    "# There are a lot of family ids, so we'll compress all of the families under 3 members into one code.\n",
    "family_ids[titanic[\"FamilySize\"] < 3] = -1\n",
    "\n",
    "# Print the count of each unique id.\n",
    "print(pandas.value_counts(family_ids))\n",
    "\n",
    "titanic[\"FamilyId\"] = family_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the best features\n",
    "\n",
    "The mission provides an example of univariate feature selection -- figuring out which features are most relevant by calculating a \"feature score\" for each feature (column inn the dataframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAEpCAYAAAByeIL3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSZIkSZIm3/8H8RXc2zrUvDcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f13d0612310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of the random forest scores 0.811447811448\n"
     ]
    }
   ],
   "source": [
    "# Code from DataQuest mission 75\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"FamilyId\"]\n",
    "\n",
    "# Perform feature selection\n",
    "selector = SelectKBest(f_classif, k=5)\n",
    "selector.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "\n",
    "# Get the raw p-values for each feature, and transform from p-values into scores\n",
    "scores = -np.log10(selector.pvalues_)\n",
    "\n",
    "# Plot the scores.  See how \"Pclass\", \"Sex\", \"Title\", and \"Fare\" are the best?\n",
    "plt.bar(range(len(predictors)), scores)\n",
    "plt.xticks(range(len(predictors)), predictors, rotation='vertical')\n",
    "plt.ylabel(\"feature scores\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Pick only the four best features.\n",
    "predictors = [\"Pclass\", \"Sex\", \"Fare\", \"Title\"]\n",
    "\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=8, min_samples_leaf=4)\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=3)\n",
    "print \"mean of the random forest scores\", scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gradient boosting and ensembling!\n",
    "\n",
    "I had heard of gradient boosting in passing from ML enthusiast friends, but I hadn't tried it in person... at a high level, the dataquest page says that the errors from one tree will help the next tree learn the dataset more effectively. There are also some suggested parameters to prevennt overftting: limiting the tree count and tree depth. \n",
    "\n",
    "The dataquest then describes ensembling -- making predictions based on several different models and averaging their results to make a final decision on what the prediction is. My reaction: ensembling seems super useful! It sounds like ensembling presents the opportunity and challenge of balancing the strengths and weaknesses of many different models; it's another layer of algorithm design and ensemble parameter tweaking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.819304152637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:40: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n"
     ]
    }
   ],
   "source": [
    "# Code from DataQuest Mission 75\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "# The algorithms we want to ensemble.\n",
    "# We're using the more linear predictors for the logistic regression, and everything with the gradient boosting classifier.\n",
    "algorithms = [\n",
    "    [GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"FamilyId\"]],\n",
    "    [LogisticRegression(random_state=1), [\"Pclass\", \"Sex\", \"Fare\", \"FamilySize\", \"Title\", \"Age\", \"Embarked\"]]\n",
    "]\n",
    "\n",
    "# Initialize the cross validation folds\n",
    "kf = KFold(titanic.shape[0], n_folds=3, random_state=1)\n",
    "\n",
    "predictions = []\n",
    "for train, test in kf:\n",
    "    train_target = titanic[\"Survived\"].iloc[train]\n",
    "    full_test_predictions = []\n",
    "    # Make predictions for each algorithm on each fold\n",
    "    for alg, predictors in algorithms:\n",
    "        # Fit the algorithm on the training data.\n",
    "        alg.fit(titanic[predictors].iloc[train,:], train_target)\n",
    "        # Select and predict on the test fold.  \n",
    "        # The .astype(float) is necessary to convert the dataframe to all floats and avoid an sklearn error.\n",
    "        test_predictions = alg.predict_proba(titanic[predictors].iloc[test,:].astype(float))[:,1]\n",
    "        full_test_predictions.append(test_predictions)\n",
    "    # Use a simple ensembling scheme -- just average the predictions to get the final classification.\n",
    "    test_predictions = (full_test_predictions[0] + full_test_predictions[1]) / 2\n",
    "    # Any value over .5 is assumed to be a 1 prediction, and below .5 is a 0 prediction.\n",
    "    test_predictions[test_predictions <= .5] = 0\n",
    "    test_predictions[test_predictions > .5] = 1\n",
    "    predictions.append(test_predictions)\n",
    "\n",
    "# Put all the predictions together into one array.\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# Compute accuracy by comparing to the training data.\n",
    "accuracy = sum(predictions[predictions == titanic[\"Survived\"]]) / len(predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing and predicting on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Code from DataQuest Mission 75\n",
    "\n",
    "# First, we'll add titles to the test set.\n",
    "titles = titanic_test[\"Name\"].apply(get_title)\n",
    "# We're adding the Dona title to the mapping, because it's in the test set, but not the training set\n",
    "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Dr\": 5, \"Rev\": 6, \"Major\": 7, \"Col\": 7, \"Mlle\": 8, \"Mme\": 8, \"Don\": 9, \"Lady\": 10, \"Countess\": 10, \"Jonkheer\": 10, \"Sir\": 9, \"Capt\": 7, \"Ms\": 2, \"Dona\": 10}\n",
    "for k,v in title_mapping.items():\n",
    "    titles[titles == k] = v\n",
    "titanic_test[\"Title\"] = titles\n",
    "\n",
    "# Check the counts of each unique title.\n",
    "# print(pandas.value_counts(titanic_test[\"Title\"]))\n",
    "\n",
    "# Now, we add the family size column.\n",
    "titanic_test[\"FamilySize\"] = titanic_test[\"SibSp\"] + titanic_test[\"Parch\"]\n",
    "\n",
    "# Now we can add family ids.\n",
    "# We'll use the same ids that we did earlier.\n",
    "# print(family_id_mapping)\n",
    "\n",
    "family_ids = titanic_test.apply(get_family_id, axis=1)\n",
    "family_ids[titanic_test[\"FamilySize\"] < 3] = -1\n",
    "titanic_test[\"FamilyId\"] = family_ids\n",
    "\n",
    "titanic_test[\"NameLength\"] = titanic_test[\"Name\"].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Code from DataQuest Mission 75\n",
    "\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"FamilyId\"]\n",
    "\n",
    "algorithms = [\n",
    "    [GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), predictors],\n",
    "    [LogisticRegression(random_state=1), [\"Pclass\", \"Sex\", \"Fare\", \"FamilySize\", \"Title\", \"Age\", \"Embarked\"]]\n",
    "]\n",
    "\n",
    "full_predictions = []\n",
    "for alg, predictors in algorithms:\n",
    "    # Fit the algorithm using the full training data.\n",
    "    alg.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "    # Predict using the test dataset.  We have to convert all the columns to floats to avoid an error.\n",
    "    predictions = alg.predict_proba(titanic_test[predictors].astype(float))[:,1]\n",
    "    full_predictions.append(predictions)\n",
    "\n",
    "# The gradient boosting classifier generates better predictions, so we weight it higher.\n",
    "predictions = (full_predictions[0] * 3 + full_predictions[1]) / 4\n",
    "\n",
    "# turning the predictions into 0s and 1s\n",
    "for i in range(predictions.shape[0]):\n",
    "    predictions[i] = (predictions[i] >= 0.5)\n",
    "\n",
    "predictions = predictions.astype(int)\n",
    "\n",
    "# Create a new dataframe with only the columns Kaggle wants from the dataset.\n",
    "submission = pandas.DataFrame({\n",
    "        \"PassengerId\": titanic_test[\"PassengerId\"],\n",
    "        \"Survived\": predictions\n",
    "    }) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save it\n",
    "submission.to_csv(\"dataquest75.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How the model performed on kaggle\n",
    "\n",
    "The model had a score of 0.79904 and is at rank 1003 on the kaggle leaderboards. It is reassuring that the changes explored in the dataquest mission resulted in a nontrivial improvement on test performance over the logistic regression models from model_iteration_1! I have a feeling that Data Science projects are neverending -- there's always some room for improvement or new combination of ideas to try and see if those new ideas work just as well or better than the current iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ideas for future work\n",
    "\n",
    "If I had more time...\n",
    "\n",
    "* Getting more familiar with scikit learn and the many algorithms it has -- ensemble lots of things!\n",
    "* Trying different ensembling techniques:\n",
    "     * instead of averaging, maybe voting (going with what the majority of the models say),\n",
    "     * the [wikipedia page on Ensemble learning](https://en.wikipedia.org/wiki/Ensemble_learning) and the [scikit learn page on Ensemble methods](http://scikit-learn.org/stable/modules/ensemble.html) -- maybe trying things like AdaBoost, the Bayes optimal classifier, etc. and plotting their performance vs. ensemble learning method\n",
    "* Following advice from this scikit learn cheat-sheet on the [Choosing the right estimator](http://scikit-learn.org/stable/tutorial/machine_learning_map/) page (the image on the webpage also includes links for the boxes in this figure):\n",
    "![scikit-learn algorithm cheat-sheet](http://scikit-learn.org/stable/_static/ml_map.png)\n",
    "* Reading more about the history of the Titanic and getting more inspiration from the details of the event (gathering more domain-specific knowledge than I currently have about the Titanic) \n",
    "* Trying different numbers of folds for the experiments and plotting the model performance vs. number of folds. There must be a sweet spot between too many folds (too little training data for each sub-trial of the model to do well...) and too little folds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ending thoughts\n",
    "\n",
    "In regards to time management... Week 2 of the semester definitely was a \"recalibration\" week for me -- I've found that I'm most productive in Data Science when I have at least 1.5 hours to work. Here's a quick bulletpoint list of the proces:\n",
    "\n",
    "* gathering resources and figuring out which tools to try using first, \n",
    "* reading, \n",
    "* implementing code, \n",
    "* jotting down commentary in real time with the markdown cells\n",
    "* debugging code, \n",
    "* learning about functionality of the tools (or perhaps investigating other tools)\n",
    "* getting creative with ideas on how to improve the model, \n",
    "* implementing those creative ideas and debugging,\n",
    "* continued...\n",
    "\n",
    "There are so many steps that need to happen before one can easily do creative changes to improve your model! Definitely worth thinking about this process with the future steps in mind. e.g. Once the first pass at implementating the data cleanup, feature engineering, and training the model is done, make functions that can be used for speedy feature engineering and testing in the creative phase. Also, take advantage of how easy it is to swap scikit-learn functions. \n",
    "\n",
    "At the beginning of this warmup project I felt like each step could be a timesink and was overwhelmed by the amount of work goes into each notebook. I also felt like I could have done more at any step in time and tried to express these ideas in the markdown cells. I've done this process before in the machine learning co-curricular but that was with a smaller time commitment and longer timespan (1 credit for DoML vs. 4 credits for Data Science). In the future I'll do a better job of breaking down tasks into smaller chunks that I can do in my relatively frequent small chunks of free time (often I have 30-60 minutes free during the workday hours) and leaving notes to myself about where I left off. In retrospect this seems like common sense with Olin workload time management but this warmup project was a helpful \"recalibration\" phase for me before the next projects start. :) \n",
    "\n",
    "I definitely value the opportunity for creativity in the \"improving your model\" phase -- next time I will try some creative exercises to generate a large quantity of ideas, and how visualizations or results will either confirm those ideas or lead to new questions. In this project I didn't devote a lot of time to do divergent thinking exercises but will try refining the creative process in Data Science in the next projects."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
